[
  {
    "objectID": "python_intermediate_regression_complete_ASRI25.html#asri-2025",
    "href": "python_intermediate_regression_complete_ASRI25.html#asri-2025",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "ASRI 2025",
    "text": "ASRI 2025\n\n\n\nRegression in Python (Intermediate)\n\n\nThe notebook uses the following modules:\n\nmatplotlib : Provides basic graphing/charting.\nnumpy : Allows matrix and vector/array math.\npandas : Provides DataFrame functionality.\nseaborn : Works with matplotlib to provide nicer graphs.\nsklearn : Scikit-Learn provides machine learning and data manipulation tools.\n\nWe will rely heavily on the Scikit-Learn library for models, metrics, and experimental design tools. See the full documentation for this fantastic library at https://scikit-learn.org."
  },
  {
    "objectID": "python_intermediate_regression_complete_ASRI25.html#first-some-terms-and-definitions",
    "href": "python_intermediate_regression_complete_ASRI25.html#first-some-terms-and-definitions",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "First, some terms and definitions:",
    "text": "First, some terms and definitions:\nRegression is the process of predicting a continuous value given the random variables for a given sample.\nContinuous values are numeric values that can take on any value within some range. Examples include height, weight, temperature, price, etc.\nA sample consists of all of the experimental information gathered for one item in the dataset. Sometimes a sample is called an object or item. Usually samples are arranged as rows in tabular datasets (CSV files, Excel spreadsheets, or similar).\nA random variable, sometimes called an input variable, measurement, or feature, is the recorded value for some property of the sample that was measured in the experiment, e.g.¬†‚Äúweight‚Äù, ‚Äúhorsepower‚Äù, ‚Äúnumber of cylinders‚Äù, etc.\n\nYou have a regression problem if the dependent variable (output value) you are trying to predict is continuous.\nWe will focus first on regression problems where the random variables are also continuous.\nAt the end, a section is provided with some tips for working with random variables that are categorical."
  },
  {
    "objectID": "python_intermediate_regression_complete_ASRI25.html#lets-see-some-code",
    "href": "python_intermediate_regression_complete_ASRI25.html#lets-see-some-code",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "Let‚Äôs see some code!",
    "text": "Let‚Äôs see some code!\nFirst, we have to import the modules, objects, and functions we will be using in this tutorial:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\nThe Dataset\nFor this tutorial, we will use the ‚ÄúAuto MPG‚Äù dataset, which is a classic dataset for regression tasks. It contains information about various automobiles, including their fuel consumption in miles per gallon (mpg, which will be our target variable to predict.\nLet‚Äôs load the dataset and take a look at it:\n\n# Define column names based on the dataset description\ncolumn_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', \n                'acceleration', 'model_year', 'origin', 'car_name']\n\n# Download the dataset from UCI ML Repository if needed.\n! [[ -f auto+mpg.zip ]] || { wget https://archive.ics.uci.edu/static/public/9/auto+mpg.zip && unzip -o auto+mpg.zip && rm Index auto-mpg.data-original; }\n\n# Load the dataset\nauto_mpg = pd.read_csv('auto-mpg.data', sep=r'\\s+', names=column_names)\n\n# Display the first few rows\nauto_mpg.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\ncar_name\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504.0\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165.0\n3693.0\n11.5\n70\n1\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150.0\n3436.0\n11.0\n70\n1\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150.0\n3433.0\n12.0\n70\n1\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140.0\n3449.0\n10.5\n70\n1\nford torino\n\n\n\n\n\n\n\nThe mpg column contains the value that we want to predict (it is our target column). We‚Äôll use the other numeric columns as random variables (predictors).\nIt will make things easier if we create variables to contain the name of the target column and the random variables. These can be used when we interact with Pandas DataFrames to quickly select those columns by name.\n\ntarget_col = \"mpg\"\nrandom_var_cols = [\n    \"cylinders\",\n    \"displacement\",\n    \"horsepower\",\n    \"weight\",\n    \"acceleration\",\n    \"model_year\"\n]\n\nLet‚Äôs use the info() DataFrame method to see what kinds of values we have, and whether there are any missing values.\n\nauto_mpg.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 398 entries, 0 to 397\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           398 non-null    float64\n 1   cylinders     398 non-null    int64  \n 2   displacement  398 non-null    float64\n 3   horsepower    398 non-null    object \n 4   weight        398 non-null    float64\n 5   acceleration  398 non-null    float64\n 6   model_year    398 non-null    int64  \n 7   origin        398 non-null    int64  \n 8   car_name      398 non-null    object \ndtypes: float64(4), int64(3), object(2)\nmemory usage: 28.1+ KB\n\n\nü§î We notice that the ‚Äòhorsepower‚Äô column is not recognized as numeric (the type is reported as ‚Äúobject‚Äù). Let‚Äôs check if there are any non-numeric values.\nOne way to do that is to look at the values. Another is to write a function to check if each value can be converted to a float, and return True if it can or False otherwise. Then, we can just filter for values that will not convert:\n\ndef is_number(x):\n    \"\"\"Returns True if `x` is a number, or False otherwise.\"\"\"\n    try:\n        float(x)\n    except ValueError:\n        return False\n    return True\n# List the values and counts for all non-numeric values in the 'horsepower' variable:\nauto_mpg.loc[auto_mpg['horsepower'].apply(is_number) == False, 'horsepower'].value_counts()\n\nhorsepower\n?    6\nName: count, dtype: int64\n\n\nFrom this, we can see that the only non-numeric value is ‚Äò?‚Äô. So, we could convert the column to numeric after replacing the ‚Äò?‚Äô values with na.nan, or we can just reload the DataFrame from the CSV file and tell Pandas to treat ‚Äò?‚Äô as NA. We will do the latter to demonstrate how to do it, and because this dataset is small enough that it will not take long to do it that way. (For larger data, the former approach would probably be faster.)\n\n# Re-load the dataset, treating '?' as NA values.\nauto_mpg = pd.read_csv('auto-mpg.data', sep=r'\\s+', names=column_names, na_values='?')\n# Check the info again\nauto_mpg.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 398 entries, 0 to 397\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           398 non-null    float64\n 1   cylinders     398 non-null    int64  \n 2   displacement  398 non-null    float64\n 3   horsepower    392 non-null    float64\n 4   weight        398 non-null    float64\n 5   acceleration  398 non-null    float64\n 6   model_year    398 non-null    int64  \n 7   origin        398 non-null    int64  \n 8   car_name      398 non-null    object \ndtypes: float64(5), int64(3), object(1)\nmemory usage: 28.1+ KB\n\n\nNow, we have correct data types. We will still have missing values in ‚Äòhorsepower‚Äô though, so we need to remove those rows:\n‚ÑπÔ∏è Pandas has a method called dropna() that can drop missing values.\nIn this case, we want to call it with the subset argument set to our random variable cols (random_var_cols) and the axis argument set to 0.\n\n# Now drop rows with missing values\nauto_mpg = auto_mpg.dropna(subset=random_var_cols, axis=0)\n\n\n\nüìä Visualize Early, Visualize Often\nLet‚Äôs take a look at the dataset. We will plot some relationships between our target variable (mpg) and the predictors. (This will take several seconds.)\n\nsns.pairplot(auto_mpg[random_var_cols + [target_col]], corner=True)\n\n\n\n\n\n\n\n\nLooking at the pairplot, we can see that there are clear relationships between MPG and the various predictors (bottom row). For example, as weight increases, MPG tends to decrease. Similarly, as horsepower and displacement increase, MPG tends to decrease. These relationships make intuitive sense: heavier cars with more powerful engines typically consume more fuel.\nWe can also see that some of the predictors have relationships with one another, which could be a problem for models that assume independent variables. Let‚Äôs get a sense of that by plotting the (Spearman) correlation between all the variables:\n\n# Calculate correlation matrix.  We use Spearman because we observed some non-linear relationships in the pairplot.\ncorrelation_matrix = auto_mpg[random_var_cols + [target_col]].corr('spearman')\n\n# Plot the correlation matrix as a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nThe correlation matrix confirms our observations from the scatter plots. We can see strong negative correlations between MPG and weight, displacement, and horsepower. This suggests that these variables will be important predictors in our regression models.\n‚ú® Choosing the right random variables for prediction is vital. This is why it is a good idea to get to know your dataset early in the process! Visualize early, visualize often!"
  },
  {
    "objectID": "python_intermediate_regression_complete_ASRI25.html#lets-see-how-well-we-can-predict-mpg-with-a-linear-model.",
    "href": "python_intermediate_regression_complete_ASRI25.html#lets-see-how-well-we-can-predict-mpg-with-a-linear-model.",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "Let‚Äôs see how well we can predict MPG with a linear model.",
    "text": "Let‚Äôs see how well we can predict MPG with a linear model.\nFirst, we‚Äôll use the LinearRegression model from scikit-learn.\nBased on the visualizations above, let‚Äôs start with a simple model using just weight and displacement as predictors:\n\nsimple_random_var_cols = [\n    \"weight\",\n    \"displacement\",\n]\n\nFor scoring, we will use R¬≤ and Mean Absolute Error (MAE).\n\nscoring_metrics = {\n    'r2': 'r2',\n    'mae': 'neg_mean_absolute_error',\n}\n\nTo quickly determine if a linear model will be suitable for this problem, we can use the cross_validate() function from Scikit-Learn. This function wraps up a lot of functionality. It will set up a k-fold cross validation experiment (with default of \\(k=5\\), for five-fold CV). It will take the model of your choice and automatically train the model for each training fold, then predict the test cases and score the predictions on the test folds (with the R¬≤ metric by default for regression).\nThe scores for each fold are returned. We can calculate and report the mean score over all five folds along with the standard deviation of the scores to see whether the model is able to do a good job in general, and how much variation we would expect for different training sets. Models should have high R¬≤ values, and a low standard deviation would indicate that the model generalizes to new data very well. (A high standard deviation would indicate the model is unstable and doesn‚Äôt generalize well.) For the Mean Absolute Error metric, we want smaller values (less deviation from the true mpg, and a small standard deviation.\nThe linear model will look like:\n\\[\ny_{mpg} = \\beta_0 + \\beta_1 x_{weight} + \\beta_2 x_{displacement}\n\\]\n\nscores = cross_validate(\n    LinearRegression(), X=auto_mpg[simple_random_var_cols], y=auto_mpg[target_col],\n    scoring=scoring_metrics\n)\nprint(\n    f\"mean R¬≤ : {scores['test_r2'].mean():0.3f}, std: {scores['test_r2'].std():0.3f}\\n\"\n    f\"mean MAE: {-scores['test_mae'].mean():0.1f} mpg, std: {scores['test_mae'].std():0.1f} mpg.\"\n)  # print mean and standard deviation of score metrics\n\nmean R¬≤ : 0.326, std: 0.523\nmean MAE: 3.8 mpg, std: 1.4 mpg.\n\n\nüòû The two-variable linear model seems pretty unstable at this task. Look closely at the R¬≤ mean: 0.326 is a pretty low R¬≤ value to begin with, and the standard deviation over the 5 runs of the cross-validation experiment was 0.523, which is larger than the mean value! That means that on average, the linear model can explain only about 33% of the variance in the data, and the high standard deviation means that the model is not generalizing very well.\nü§î We might have expected this if we look back at our pair plots of weight and displacement versus mpg. Do you see the ‚Äúcurve‚Äù to the scatter? That is a good indicator that a linear model might not be ideal for this task.\nLet‚Äôs scale the model up to use all of our predictors.\nFor now, we will stick with a linear model. But, let‚Äôs use all of our random variables to see if that improves the result.\n\nscores = cross_validate(\n    LinearRegression(), X=auto_mpg[random_var_cols], y=auto_mpg[target_col],\n    scoring=scoring_metrics\n)\nprint(\n    f\"mean R¬≤ : {scores['test_r2'].mean():0.3f}, std: {scores['test_r2'].std():0.3f}\\n\"\n    f\"mean MAE: {-scores['test_mae'].mean():0.1f} mpg, std: {scores['test_mae'].std():0.1f} mpg.\"\n)  # print mean and standard deviation of score metrics\n\nmean R¬≤ : 0.571, std: 0.231\nmean MAE: 3.1 mpg, std: 0.8 mpg.\n\n\nThis provided a modest improvement, both in R¬≤ and standard deviation. We see a smaller change in MAE and its standard deviation, but still an improvement nonetheless.\nLet‚Äôs try a non-linear model. Let‚Äôs consider a quadratic model (a polynomial model of degree 2). The model will look like:\n\\[\ny_{mpg} = \\beta_0 + \\beta_1 x_{weight} + \\beta_2 x_{displacement} + \\beta_3 x_{weight}^2 + \\beta_4 x_{displacement}^2\n\\]\nThe way we do this is to pre-compute the polynomial features \\(x_{weight}^2\\) and \\(x_{displacement}^2\\), then use a linear regression model as before.\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nscores = cross_validate(\n    LinearRegression(), X=poly.fit_transform(auto_mpg[random_var_cols]), y=auto_mpg[target_col],\n    scoring=scoring_metrics\n)\nprint(\n    f\"mean R¬≤ : {scores['test_r2'].mean():0.3f}, std: {scores['test_r2'].std():0.3f}\\n\"\n    f\"mean MAE: {-scores['test_mae'].mean():0.1f} mpg, std: {scores['test_mae'].std():0.1f} mpg.\"\n)  # print mean and standard deviation of score metrics\n\nmean R¬≤ : 0.679, std: 0.222\nmean MAE: 2.6 mpg, std: 1.0 mpg.\n\n\nüéâ Now we see a better result. R¬≤ above 65% is starting to look more promising (but not necessarily ‚Äúgood‚Äù). We see that on average our model is off by about 2.6 mpg. If that is an acceptable error amount, we might be happy with this one.\nNow, let‚Äôs take a look at a different kind of model, just for comparison. A Random Forest model is a non-linear model that works well for lots of tasks. Scikit-Learn provides one called RandomForestRegressor for regression problems.\nLet‚Äôs try it in exactly the same experimental setup we used for the linear model.\n\nscores = cross_validate(\n    RandomForestRegressor(random_state=1),\n    X=auto_mpg[random_var_cols],\n    y=auto_mpg[target_col],\n    scoring=scoring_metrics\n)\nprint(\n    f\"mean R¬≤ : {scores['test_r2'].mean():0.3f}, std: {scores['test_r2'].std():0.3f}\\n\"\n    f\"mean MAE: {-scores['test_mae'].mean():0.1f} mpg, std: {scores['test_mae'].std():0.1f} mpg.\"\n)  # print mean and standard deviation of score metrics\n\nmean R¬≤ : 0.748, std: 0.198\nmean MAE: 2.2 mpg, std: 0.8 mpg.\n\n\nThe random forest did even better than the quadratic model! This suggests that there might be non-linear relationships in the data that the random forest is able to capture better than either the linear or quadratic models.\nOne note:\nWe used random_state=1 to seed the random number generator within the model, causing it to produce identical results if we train it again on the same data. Random forests (as implied by their name) rely on some randomness during training, so you don‚Äôt expect to get the same performance every time. This makes reproducible results difficult.\nüí° By seeding the random state, we ‚Äúlock‚Äù it to a specific outcome (assuming no external changes). This way, others can reproduce our results in the future."
  },
  {
    "objectID": "python_intermediate_regression_complete_ASRI25.html#exploring-more-ways-to-characterize-regressor-performance.",
    "href": "python_intermediate_regression_complete_ASRI25.html#exploring-more-ways-to-characterize-regressor-performance.",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "Exploring more ways to characterize regressor performance.",
    "text": "Exploring more ways to characterize regressor performance.\n\nüìä Visualize!\nWhen evaluating regression models, it‚Äôs important to look at the residuals (the differences between predicted and actual values). A good regression model should have residuals that are randomly distributed around zero.\nLet‚Äôs split our data into training and testing sets, train our models, and then visualize the residuals:\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    auto_mpg[random_var_cols], auto_mpg[target_col], test_size=0.2, random_state=42\n)\n\n# Train the linear regression model\nlinear_model = LinearRegression().fit(X_train, y_train)\n\n# Make predictions on the test set\nlinear_preds = linear_model.predict(X_test)\n\n# Calculate residuals\nlinear_residuals = y_test - linear_preds\n\n# Plot the residuals\nplt.figure(figsize=(10, 6))\nplt.scatter(linear_preds, linear_residuals)\nplt.axhline(y=0, color='r', linestyle='-')\nplt.xlabel('Predicted MPG')\nplt.ylabel('Residuals')\nplt.title('Residual Plot for Linear Regression')\nplt.show()\n\n\n\n\n\n\n\n\n‚ú® Interpreting a residual plot: To interpret a residual plot like the one above, we look at two things: (1) The magnitude of the residuals, which represents how far our predictions are from the actual values. We want the dots to be close to the zero line. (2) The shape of the residuals. The plotted residuals are ordered by magnitude of the prediction from smallest to largest, covering the range of predicted values. What we want is to see no trend or ‚Äúpattern‚Äù to the scatter of residuals versus the zero line. If we see a trend or pattern, then it is a clue that our model is not making the same mistakes across the range of its outputs, and so it might not be a good fit for the application.\nHere, we see that there is a ‚Äúcurve‚Äù in the residuals‚Äîthey start above the zero line, trend downward, then back up again (a ‚Äúsmile‚Äù pattern). This is a clue that the actual target variable ‚Äúmpg‚Äù is probably not a linear function of the predictors. A non-linear model might work better.\nNow let‚Äôs do the same plot for the Random Forest model:\n\n# Train the random forest model\nrf_model = RandomForestRegressor(random_state=1).fit(X_train, y_train)\n\n# Make predictions on the test set\nrf_preds = rf_model.predict(X_test)\n\n# Calculate residuals\nrf_residuals = y_test - rf_preds\n\n# Plot the residuals\nplt.figure(figsize=(10, 6))\nplt.scatter(rf_preds, rf_residuals)\nplt.axhline(y=0, color='r', linestyle='-')\nplt.xlabel('Predicted MPG')\nplt.ylabel('Residuals')\nplt.title('Residual Plot for Random Forest')\nplt.show()\n\n\n\n\n\n\n\n\nHere, we don‚Äôt see as much of a pattern. That is a good thing. For the most part, our Random Forest model seems to be making similar errors across its range, with two possible exceptions: The smaller predictions seem to be better than the larger ones in general, and there is a rough patch between about 22 and 32 mpg. where the model seems to be over-predicting more than everywhere else. An even more powerful model might be able to do a better job, but we won‚Äôt investigate that in this workshop.\nüí° Another way to visualize the errors: Let‚Äôs also compare the actual vs.¬†predicted values for both models:\n\n# Create a figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot actual vs predicted for Linear Regression\nax1.scatter(y_test, linear_preds)\nax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nax1.set_xlabel('Actual MPG')\nax1.set_ylabel('Predicted MPG')\nax1.set_title('Linear Regression: Actual vs Predicted')\n\n# Plot actual vs predicted for Random Forest\nax2.scatter(y_test, rf_preds)\nax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nax2.set_xlabel('Actual MPG')\nax2.set_ylabel('Predicted MPG')\nax2.set_title('Random Forest: Actual vs Predicted')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: To interpret these plots, keep in mind that the dotted red line represents a ‚Äúperfect fit‚Äù model. We don‚Äôt expect every dot to be on the line, but we want them close and randomly spread around it (no patterns). On the left plot (the linear model), we see a ‚Äúbent‚Äù scatter of points compared to the line, indicating a bad fit. On the right, the Random Forest (RF) behaves better, but we can see the over-prediction in the mid-range and then the beginnings of under-prediction happing at the top end (similar to what we see on the linear model‚Äôs graph). Again, the RF looks better, but leaves room for improvement.\n\n\nNon-visual metrics\nLet‚Äôs look at other regression metrics.\nScikit-Learn provides several metrics appropriate for evaluating regression models. You can see the list at https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics.\nWe‚Äôll calculate some common metrics for both models:\n\n# Calculate metrics for Linear Regression\nlinear_mse = mean_squared_error(y_test, linear_preds)\nlinear_rmse = np.sqrt(linear_mse)\nlinear_mae = mean_absolute_error(y_test, linear_preds)\nlinear_msd = np.mean(linear_preds-y_test)\nlinear_r2 = r2_score(y_test, linear_preds)\n\n# Calculate metrics for Random Forest\nrf_mse = mean_squared_error(y_test, rf_preds)\nrf_rmse = np.sqrt(rf_mse)\nrf_mae = mean_absolute_error(y_test, rf_preds)\nrf_msd = np.mean(rf_preds-y_test)\nrf_r2 = r2_score(y_test, rf_preds)\n\n# Create a DataFrame to display the metrics\nmetrics_df = pd.DataFrame({\n    'Model': ['Linear Regression', 'Random Forest'],\n    'MSE': [linear_mse, rf_mse],\n    'RMSE': [linear_rmse, rf_rmse],\n    'MAE': [linear_mae, rf_mae],\n    'MSD': [linear_msd, rf_msd],\n    'R¬≤': [linear_r2, rf_r2]\n})\n\nmetrics_df\n\n\n\n\n\n\n\n\nModel\nMSE\nRMSE\nMAE\nMSD\nR¬≤\n\n\n\n\n0\nLinear Regression\n10.502370\n3.240736\n2.503860\n0.179652\n0.794235\n\n\n1\nRandom Forest\n5.876812\n2.424214\n1.732253\n0.409367\n0.884860\n\n\n\n\n\n\n\nLet‚Äôs understand these metrics:\n\nMean Squared Error (MSE): The average of the squared differences between predicted and actual values. Lower is better.\nRoot Mean Squared Error (RMSE): The square root of MSE. It‚Äôs in the same units as the target variable, making it more interpretable. Lower is better.\nMean Absolute Error (MAE): The average of the absolute differences between predicted and actual values. Lower is better.\nMean Signed Deviation (MSD): The average of the differences between predicted and actual values, retaining the sign. Closer to zero is better, and the sign indicates the direction of the bias (e.g.¬†model is over-predicting vs.¬†under-predicting).\nR¬≤ (Coefficient of Determination): Represents the proportion of variance in the dependent variable that is predictable from the independent variables. Ranges from 0 to 1, with higher values indicating better fit.\n\nBased on these metrics, the Random Forest model outperforms the Linear Regression model on our test set. However, MSD reveals that the RF model is a bit more biased toward over-predicting the mpg than the linear model. So, when RF makes a mistake, it is more likely to favor higher mpg.\n\n\nFeature Importance\nSome models can provide information about the importance of each featuer, allowing us to understand the underlying process better and perhaps perform feature selection to simplify our models. Different models do this differently.\nLinear Regression models convey feature importance in the magnitude of the coefficients they compute for each feature. You can access this information for Scikit-Learn LinearRegression models in the coef_ attribute:\n\n# We will get the absolute values of the coefficients, since we don't care about direction here\nlinear_feature_importance = np.abs(linear_model.coef_)\n# And then normalize them so that they sum to 1.0 just to make the scale easier to interpret.\nlinear_feature_importance /= np.sum(linear_feature_importance)\n\n# Create a DataFrame to display feature importance\nlinear_importance_df = pd.DataFrame({\n    'Feature': random_var_cols,\n    'Importance': linear_feature_importance\n})\n\n# Sort by importance\nlinear_importance_df = linear_importance_df.sort_values('Importance', ascending=False)\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=linear_importance_df)\nplt.title('Feature Importance from Linear Regression Model')\nplt.xlabel(\"Relative Importance\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nüíπ We can see from this that ‚Äúmodel_year‚Äù had the most impact on the linear model. Looking back at the original scatterplots, we can see that model year does have a mostly-linear trend where mpg tends to increase in more recent years.\nThe RandomForestRegressor model provides feature importance information in its feature_importances_ attribute:\n\n# Get feature importances from the Random Forest model (it is already normalized to sum to 1.0).\nrf_feature_importance = rf_model.feature_importances_\n\n# Create a DataFrame to display feature importance\nimportance_df = pd.DataFrame({\n    'Feature': random_var_cols,\n    'Importance': rf_feature_importance\n})\n\n# Sort by importance\nimportance_df = importance_df.sort_values('Importance', ascending=False)\n\n# Plot feature importances\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=importance_df)\nplt.title('Feature Importances from Random Forest Model')\nplt.xlabel(\"Relative Importance\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nü§î Notice that the RF model has a different conclusion about which feature is most important, choosing ‚Äúdisplacement‚Äù. Displacement seemed highly correlated with mpg in the original scatterplot, but the relationship is highly non-linear, and has redundancy with ‚Äúcylinders‚Äù and ‚Äúhorsepower‚Äù. The RF was able to use this variable in spite of these challenges."
  },
  {
    "objectID": "python_intermediate_regression_complete_ASRI25.html#working-with-categorical-features",
    "href": "python_intermediate_regression_complete_ASRI25.html#working-with-categorical-features",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "Working with categorical features",
    "text": "Working with categorical features\nSo far, we‚Äôve only used numeric features for our predictors. But the Auto MPG dataset also contains a categorical feature:\n\norigin - three levels: [1, 2, 3] representing American, European, and Japanese cars respectively\n\nLet‚Äôs convert this numeric encoding to more meaningful labels first:\n\n# Create a mapping dictionary\norigin_map = {1: 'American', 2: 'European', 3: 'Japanese'}\n\n# Create a new column with the mapped values\nauto_mpg['origin_name'] = auto_mpg['origin'].map(origin_map)\n\n# Display the first few rows to verify\nauto_mpg[['origin', 'origin_name']].head()\n\n\n\n\n\n\n\n\norigin\norigin_name\n\n\n\n\n0\n1\nAmerican\n\n\n1\n1\nAmerican\n\n\n2\n1\nAmerican\n\n\n3\n1\nAmerican\n\n\n4\n1\nAmerican\n\n\n\n\n\n\n\nNow let‚Äôs visualize how MPG varies by origin:\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='origin_name', y='mpg', data=auto_mpg)\nplt.title('MPG by Car Origin')\nplt.xlabel('Origin')\nplt.ylabel('MPG')\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that there are differences in MPG based on the car‚Äôs origin. Japanese cars tend to have higher MPG, followed by European cars, with American cars having the lowest MPG on average.\nTo include this categorical variable in our regression models, we need to encode it. One common approach is one-hot encoding.\nOne-hot encoding is an encoding technique in which a variable with \\(N\\) levels is split into \\(N\\) new pseudo-variables where each is a binary variable encoded as 1 or 0.\nLet‚Äôs see how our origin variable might look if it were one-hot encoded:\nBefore\nmpg  cylinders  displacement  ...  origin  car_name\n20.5  6         200.0         ...  1       chevrolet malibu\n15.0  8         350.0         ...  1       buick skylark 320\n22.0  4         121.0         ...  2       volkswagen 411 (sw)\n26.0  4         98.00         ...  2       fiat 124 sport coupe\n32.0  4         71.00         ...  3       toyota corolla 1200\n24.0  4         120.0         ...  3       honda civic\nAfter\nmpg  cylinders  displacement  ...  origin_American  origin_European  origin_Japanese  car_name\n20.5  6         200.0         ...  1                0                0                chevrolet malibu\n15.0  8         350.0         ...  1                0                0                buick skylark 320\n22.0  4         121.0         ...  0                1                0                volkswagen 411 (sw)\n26.0  4         98.00         ...  0                1                0                fiat 124 sport coupe\n32.0  4         71.00         ...  0                0                1                toyota corolla 1200\n24.0  4         120.0         ...  0                0                1                honda civic\nHere‚Äôs the code:\nPandas can do this in a dataframe by using the get_dummies() method. You provide a prefix (like \"origin\") and the existing levels are used to complete the new column names.\n\nsample_rows = [253, 1, 77, 114, 131, 149] # this lets us select the same cars shown above\nauto_mpg_encoded = pd.get_dummies(        # get_dummies converts to one-hot encoding\n    auto_mpg, columns=[\"origin_name\"], prefix=\"origin\", dtype=int\n)\nauto_mpg_encoded.loc[sample_rows][['mpg', 'origin_American', 'origin_European', 'origin_Japanese', 'car_name']]\n\n\n\n\n\n\n\n\nmpg\norigin_American\norigin_European\norigin_Japanese\ncar_name\n\n\n\n\n253\n20.5\n1\n0\n0\nchevrolet malibu\n\n\n1\n15.0\n1\n0\n0\nbuick skylark 320\n\n\n77\n22.0\n0\n1\n0\nvolkswagen 411 (sw)\n\n\n114\n26.0\n0\n1\n0\nfiat 124 sport coupe\n\n\n131\n32.0\n0\n0\n1\ntoyota corolla 1200\n\n\n149\n24.0\n0\n0\n1\nhonda civic\n\n\n\n\n\n\n\nNow let‚Äôs use these one-hot encoded features in our regression models:\n\n# Define columns including one-hot encoded origin\nall_features = random_var_cols + ['origin_American', 'origin_European', 'origin_Japanese']\n\nNow we will evaluate each of our models with a 5-fold CV like we did in the beginning, and compare the original features to the ones with origin information included:\n\n# create a helper function so we don't have to repeat this code too much\ndef do_evaluation(model, X, y, caption):\n    scores = cross_validate( model, X, y, scoring=scoring_metrics)\n    print(\n        f\"{caption}\\n\"\n        f\"mean R¬≤ : {scores['test_r2'].mean():0.3f}, std: {scores['test_r2'].std():0.3f}\\n\"\n        f\"mean MAE: {-scores['test_mae'].mean():0.1f} mpg, std: {scores['test_mae'].std():0.2f} mpg.\\n\"\n    )  # print mean and standard deviation of score metrics\n\n\ndo_evaluation(LinearRegression(), X=auto_mpg[random_var_cols], y=auto_mpg[target_col], caption=\"Linear Regression without origin info:\")\ndo_evaluation(LinearRegression(), X=auto_mpg_encoded[all_features], y=auto_mpg_encoded[target_col], caption=\"Linear Regression including origin info:\")\n\nprint(\"---\\n\")\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\ndo_evaluation(LinearRegression(), X=poly.fit_transform(auto_mpg[random_var_cols]), y=auto_mpg[target_col], caption=\"Polynomial (d=2) Regression without origin info:\")\ndo_evaluation(LinearRegression(), X=poly.fit_transform(auto_mpg_encoded[all_features]), y=auto_mpg_encoded[target_col], caption=\"Polynomial (d=2) Regression including origin info:\")\n\nprint(\"---\\n\")\n\ndo_evaluation(RandomForestRegressor(random_state=1), X=auto_mpg[random_var_cols], y=auto_mpg[target_col], caption=\"Random Forest regression without origin info:\")\ndo_evaluation(RandomForestRegressor(random_state=1), X=auto_mpg_encoded[all_features], y=auto_mpg_encoded[target_col], caption=\"Random Forest regression including origin info:\")\n\nLinear Regression without origin info:\nmean R¬≤ : 0.571, std: 0.231\nmean MAE: 3.1 mpg, std: 0.76 mpg.\n\nLinear Regression including origin info:\nmean R¬≤ : 0.594, std: 0.197\nmean MAE: 3.0 mpg, std: 0.68 mpg.\n\n---\n\nPolynomial (d=2) Regression without origin info:\nmean R¬≤ : 0.679, std: 0.222\nmean MAE: 2.6 mpg, std: 0.97 mpg.\n\nPolynomial (d=2) Regression including origin info:\nmean R¬≤ : 0.661, std: 0.210\nmean MAE: 2.6 mpg, std: 0.85 mpg.\n\n---\n\nRandom Forest regression without origin info:\nmean R¬≤ : 0.748, std: 0.198\nmean MAE: 2.2 mpg, std: 0.76 mpg.\n\nRandom Forest regression including origin info:\nmean R¬≤ : 0.752, std: 0.194\nmean MAE: 2.2 mpg, std: 0.77 mpg.\n\n\n\nIncluding the origin as a categorical feature has improved our model performance in the linear regression and quadratic models, but not significantly. The RF model was mostly unchanged by the addition of this variable.\nHowever, categorical variables can be very important in some datasets! Consider encoding them and using them in the model; you can evaluate whether or not it was worthwhile before training your final model version.\n\nThere‚Äôs More Than One Way to Do It\nYou can also use the OneHotEncoder from Scikit-Learn to encode categorical variables. It‚Äôs particularly useful when you don‚Äôt want to modify your original dataframe and prefer to create a ‚Äúdata pipeline‚Äù for preprocessing your data during training or inference.\nIn fact, there are several other approaches to encoding categorical values.\nYou can learn a lot more here: https://www.kaggle.com/code/arashnic/an-overview-of-categorical-encoding-methods"
  },
  {
    "objectID": "python_intermediate_regression_complete_ASRI25.html#thank-you",
    "href": "python_intermediate_regression_complete_ASRI25.html#thank-you",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "Thank You!",
    "text": "Thank You!\nThis notebook in tutorial and completed form is available at:\nhttps://jcausey-astate.github.io/ASRI-2025/"
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI25.html#asri-2025",
    "href": "python_intermediate_classification_complete_ASRI25.html#asri-2025",
    "title": "Classification in Python (Intermediate)",
    "section": "ASRI 2025",
    "text": "ASRI 2025\n\n\n\nClassification in Python (Intermediate)\n\n\n\n\n\nThe Palmer Archipelago penguins. Artwork by @allison_horst.\n\n\nThe notebook uses the following modules:\n\nmatplotlib : Provides basic graphing/charting.\nnumpy : Allows matrix and vector/array math.\npandas : Provides DataFrame functionality.\nseaborn : Works with matplotlib to provide nicer graphs.\nsklearn : Scikit-Learn provides machine learning and data manipulation tools.\n\nWe will rely heavily on the Scikit-Learn library for models, metrics, and experimental design tools. See the full documentation for this fantastic library at https://scikit-learn.org."
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI25.html#first-some-terms-and-definitions",
    "href": "python_intermediate_classification_complete_ASRI25.html#first-some-terms-and-definitions",
    "title": "Classification in Python (Intermediate)",
    "section": "First, some terms and definitions:",
    "text": "First, some terms and definitions:\nClassification is the process of determining a categorical label given the random variables for a given sample.\nCategorical values are allowed to take on only a finite (usually small) set of values. Categorical variables are usually non-numeric, but are sometimes encoded as numbers. Sometimes we refer to values of this type as labels, factors, or classes.\nA sample consists of all of the experimental information gathered for one item in the dataset. Sometimes a sample is called an object or item. Usually samples are arranged as rows in tabular datasets (CSV files, Excel spreadsheets, or similar).\nA random variable, sometimes called an input variable, measurement, or feature, is the recorded value for some property of the sample that was measured in the experiment, e.g.¬†‚Äúheight‚Äù, ‚Äúage‚Äù, ‚Äúflower color‚Äù, etc.\n\nYou have a classification problem if the dependent variable (output value) you are trying to predict is categorical.\nWe will focus first on classification problems where the random variables are continuous.\nContinuous values are numeric values that are allowed to take on any value within some range.\nAt the end, a section is provided with some tips for working with random variables that are categorical."
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI25.html#lets-see-some-code",
    "href": "python_intermediate_classification_complete_ASRI25.html#lets-see-some-code",
    "title": "Classification in Python (Intermediate)",
    "section": "Let‚Äôs see some code!",
    "text": "Let‚Äôs see some code!\nFirst, we have to import the modules, objects, and functions we will be using in this tutorial:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nThe Dataset\n‚ÑπÔ∏è The seaborn package has some sample datasets included.\nFor this tutorial, we will use the ‚ÄúPalmer Penguins‚Äù dataset, which is called penguins in the Seaborn index. We can load it with the load_dataset() function. It will load up as a Pandas DataFrame.\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n    \n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThe species column contains the value that we want to predict (it is our label column). Although we could use all the other columns as random variables (predictors), we will only focus on the numeric values for this part of the tutorial.\nIt will make things easier if we create variables to contain the name of the label column and the random variables. These can be used when we interact with Pandas DataFrames to quickly select those columns by name. This way, we don‚Äôt have to type the list of names often, and we don‚Äôt have to create a different data structure that only contains our variables of interest (although you could also do it that way).\n\nlabel_col = \"species\"\nrandom_var_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\",\n]\n\nLet‚Äôs use the info() DataFrame method to see what kinds of values we have, and whether there are any missing values.\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\nNotice that there are some missing values. We care most about the numeric columns for this example, so we want to drop any rows with missing values in those columns.\nThe dropna() method can do this. The subset parameter lets us specify which columns we care about (the random variables we specified earlier). We use axis=0 to indicate that we want to drop rows, not columns.\n\npenguins = penguins.dropna(subset=random_var_cols, axis=0)\n\n\n\nüìä Visualize Early, Visualize Often\nLet‚Äôs take a look at the dataset. We will plot two different ‚Äòviews‚Äô for comparison. the first will compare bill length with bill depth, and the second will compare bill length with flipper length.\nWe can color the datapoints according to species so that we can visually see how separable the different classes might be.\n\n# create a figure and two subplots\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\nplt.suptitle(\"Separating penguin species by bill measurements and/or flipper length.\")\n\n# create first scatterplot using Seaborn\nsns.scatterplot(\n    data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", ax=ax1\n)\nax1.set_title(\"Bill Length vs Bill Depth (mm)\")\n\n# create second scatterplot just like the first, but with different columns\nsns.scatterplot(\n    data=penguins, x=\"bill_length_mm\", y=\"flipper_length_mm\", hue=\"species\", ax=ax2\n)\nax2.set_title(\"Bill Length vs Flipper Length (mm)\")\n\n# adjust spacing between subplots\nplt.subplots_adjust(wspace=0.15)\n\n# show the plots\nplt.show()\n\n\n\n\n\n\n\n\nIf we look at these plots, it seems we can probably do a pretty good job of separating the three classes. We see that you could even get pretty good performance by drawing a few lines to separate the groups (in other words, a simple linear model might work reasonably well).\nTo see what a harder classification problem might look like, let‚Äôs draw another scatterplot where we compare the flipper length and the body mass:\n\nsns.scatterplot(data=penguins, x=\"flipper_length_mm\", y=\"body_mass_g\", hue=\"species\")\nplt.title(\"A more difficult problem...\")\nplt.show()\n\n\n\n\n\n\n\n\nIn this plot, it is very hard to see how we could separate the ‚ÄúAdelie‚Äù group from the ‚ÄúChinstrap‚Äù group. There is even some mixing between the ‚ÄúChinstrap‚Äù and ‚ÄúGentoo‚Äù groups.\n‚ú® Choosing the right random variables for prediction is vital. This is why it is a good idea to get to know your dataset early in the process! Visualize early, visualize often!"
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI25.html#lets-see-how-well-we-can-classify-with-a-linear-model.",
    "href": "python_intermediate_classification_complete_ASRI25.html#lets-see-how-well-we-can-classify-with-a-linear-model.",
    "title": "Classification in Python (Intermediate)",
    "section": "Let‚Äôs see how well we can classify with a linear model.",
    "text": "Let‚Äôs see how well we can classify with a linear model.\nFirst, we examine the LogisticRegression model (which is actually a classification model ‚Äì don‚Äôt let the name fool you).\nBased on the graphs we plotted above, let‚Äôs use the bill length and depth as our random variables. (‚ÑπÔ∏è : We could absolutely use all four random variables and it would probably do better, but using just two gives us a chance to discuss the performance with a very simple model.)\n\nrandom_var_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n]\n\nTo quickly determine if it will be suitable to this problem, we can use the cross_val_score() function from Scikit-Learn. This function wraps up a lot of functionality. It will set up a k-fold cross validation experiment (with default of \\(k=5\\), for five-fold CV). It will take the model of your choice and automatically train the model for each training fold, then predict the test cases and score the predictions on the test folds (with the accuracy metric by default).\nThe scores for each fold are returned. We can calculate and report the mean score over all five folds along with the standard deviation of the scores to see whether the model is able to do a good job in general, and how much variation we would expect for different training sets. Models should have high accuracy, and a low standard deviation would indicate that the model generalizes to new data very well. (A high standard deviation would indicate the model is unstable and doesn‚Äôt generalize well.)\n\nscores = cross_val_score(\n    LogisticRegression(max_iter=500), X=penguins[random_var_cols], y=penguins[label_col]\n)\nprint(\n    f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\"\n)  # print mean and standard deviation\n\nmean: 0.962, std: 0.020\n\n\nüéâ Wow! The linear model does a really good job on this problem!\nOK, that isn‚Äôt really surprising since we looked at the data first and we could see that some combinations of our random variables provided good linear separation between the groups. Still, it‚Äôs nice to see our intuition was correct.\nLet‚Äôs take a look at a different kind of model, just for comparison. A Random Forest model is a non-linear model that works well for lots of tasks. Scikit-Learn provides one called RandomForestClassifier.\nLet‚Äôs try it in exactly the same experimental setup we used for the linear model.\n\nscores = cross_val_score(\n    RandomForestClassifier(random_state=1),\n    X=penguins[random_var_cols],\n    y=penguins[label_col],\n)\nprint(f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\")\n\nmean: 0.968, std: 0.017\n\n\nHere, the random forest did about the same as the linear model (especially if we take the standard deviations of scores into account).\nIf we wanted to pick between these two models for this problem, we should probably choose the simpler one ‚Äì the logistic regression model.\nüí° The Principle of Parsimony says that given the choice between multiple models with similar performance, the best choice is usually the simplest model.\n‚ÑπÔ∏è One note:\nWe used random_state=1 to seed the random number generator within the model, causing it to produce identical results if we train it again on the same data. Random forests (as implied by their name) rely on some randomness during training, so you don‚Äôt expect to get the same performance every time. This makes reproducible results difficult.\nüí° By seeding the random state, we ‚Äúlock‚Äù it to a specific outcome (assuming no external changes). This way, others can reproduce our results in the future."
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI25.html#exploring-more-ways-to-characterize-classifier-performance.",
    "href": "python_intermediate_classification_complete_ASRI25.html#exploring-more-ways-to-characterize-classifier-performance.",
    "title": "Classification in Python (Intermediate)",
    "section": "Exploring more ways to characterize classifier performance.",
    "text": "Exploring more ways to characterize classifier performance.\n\nüìä Visualize!\nWhen the model is making incorrect predictions, sometimes we want to know which samples the model predicts incorrectly. This can help us diagnose whether the model is doing the best it can, whether the model is doing strange things, or even whether there might be a problem with the dataset itself.\nGenerally, a good starting point to diagnosing the mis-predicted values from a model is for us to visualize them in some way. Since this problem is easy to visualize as a 2-D scatterplot, we will use that as a way to see which samples the model is getting right vs.¬†wrong.\nTo start, let‚Äôs just split the dataset into a simple 80% / 20% train / test split. That means that we will reserve 20% of the samples for the test set, and the other 80% will be used for training. Scikit-Learn has a simple function for doing this (train_test_split()).\nThe function returns a training and testing dataframe (or matrix) given the original full dataset and the fraction you want to hold out for the test set.\n\ntrain_df, test_df = train_test_split(penguins, test_size=0.20, random_state=2)\n\nWe‚Äôll use the LogisticRegression model again, training it on the ‚Äútrain‚Äù partition (using the fit() method). Then, we‚Äôll predict the ‚Äútest‚Äù samples and calculate the (balanced) accuracy score.\n\nmodel = LogisticRegression(max_iter=500).fit(\n    X=train_df[random_var_cols], y=train_df[label_col]\n)\npreds = model.predict(test_df[random_var_cols])\nground_truth = test_df[label_col]\nprint(f\"{balanced_accuracy_score(ground_truth, preds):0.3f}\")\n\n0.932\n\n\nNow, let‚Äôs create the scatterplot that will show which samples were predicted incorrectly.\nWe can use color to indicate correct (green) and incorrect (red) predictions. We will also use different marker shapes to indicate the true class label so that we can see which ones are being predicted incorrectly and get a sense for why.\n\nplt.figure()\nfig = sns.scatterplot(\n    data=test_df,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    hue=(preds == ground_truth),\n    style=list(test_df[label_col].values),\n    markers=[\"o\", \"D\", \"s\"],\n    palette=[\"red\", \"green\"],\n)\nfig.legend()  # weird kludge:  The \"species\" title is shown by default, but just calling `legend()` removes it.  Why? ü§∑\nplt.title(\"Correct (green) and Incorrect (red) predictions.\")\nprint(test_df[label_col].unique())\n\n['Gentoo' 'Adelie' 'Chinstrap']\n\n\n\n\n\n\n\n\n\nThe green dots are samples that were correctly predicted and the red dots are incorrect predictions. There are only four incorrect predictions. Three of those are near the ‚Äúborder‚Äù between the two visual ‚Äúclusters‚Äù. That makes sense ‚Äì class mixing obviously occurs here. The other one is a Gentoo penguin that was incorrectly predicted (probably as a Chinstrap, since there are Chinstrap penguins nearby).\nAt this point, we will make the problem harder. Why? Well, it will be more interesting to explore correct / incorrect predictions if the model is not quite so good.\nThe flipper length and body mass do not combine to give very good separation, so we will choose those as our random variables from this point forward.\nü§î Of course, we would never choose worse predictors in any real analysis, but doing this can be useful as a learning exercise.\n\nrandom_var_cols = [\"flipper_length_mm\", \"body_mass_g\"]\n\nLet‚Äôs see how our new random variables perform with the same linear model as before:\n\nscores = cross_val_score(\n    LogisticRegression(max_iter=500), X=penguins[random_var_cols], y=penguins[label_col]\n)\nprint(f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\")\n\nmean: 0.690, std: 0.061\n\n\nMuch worse performance! üò¶ That‚Äôs bad‚Ä¶ But more interesting for exploring the performance metrics.\n\nscores = cross_val_score(\n    RandomForestClassifier(random_state=1),\n    X=penguins[random_var_cols],\n    y=penguins[label_col],\n)\nprint(f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\")\n\nmean: 0.772, std: 0.041\n\n\nThe random forest did quite a bit better here. We saw that these two variables don‚Äôt provide an obvious path for linear separation, but the random forest is not limited to linear decision boundaries.\nWe will use our simple 80%/20% train/test split from earlier and train the linear model using the new (worse) combination of random variables. First, train the model:\n\nmodel = LogisticRegression(max_iter=500).fit(\n    X=train_df[random_var_cols], y=train_df[label_col]\n)\npreds = model.predict(test_df[random_var_cols])\nground_truth = test_df[label_col]\nprint(f\"{balanced_accuracy_score(ground_truth, preds):0.3f}\")\n\n0.716\n\n\n\n\nConfusion Matrix\nNow, we can use another visualization technique to discuss the performance characteristics. This technique is called a confusion matrix. It shows the number of samples from each true label that were predicted as each possible output label. Seaborn makes very nice confusion matrix plots.\n\ncm = confusion_matrix(ground_truth, preds)\n\n# plot the confusion matrix using seaborn heatmap\nsns.set(font_scale=1.4)  # adjust font size\nlabels = model.classes_\nsns.heatmap(\n    cm, annot=True, fmt=\"g\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels\n)\n\n# add axis labels and title\nplt.xlabel(\"Predicted label\")\nplt.ylabel(\"True label\")\nplt.title(\"Confusion Matrix\")\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\nCorrect predictions appear along the diagonal (upper-left to lower-right). All of the other squares represent incorrect predictions.\nHere, the linear model did very well with predicting Gentoo penguins when it saw a real Gentoo. But it also incorrectly guessed that 3 Chinstraps and 1 Adelie were Gentoos as well (False Positives).\nAs for the Chinstraps, we correctly classified 6 of them, but we incorrectly labeled 7 Chinstraps as Adelie and 3 as Gentoo (False Negatives).\n\n\nNon-visual metrics\nLet‚Äôs look at other classification metrics.\nScikit-Learn provides several metrics appropriate for evaluating classification models. You can see the list at https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics.\nWe will start with the classification_report() function, which combines several popular metrics into a single report.\n\n# We get precision, recall, and f1-score from the classification report.\n# You can also get these individually from functions in sklearn.metrics.\nprint(classification_report(ground_truth, preds))\n\n              precision    recall  f1-score   support\n\n      Adelie       0.77      0.77      0.77        31\n   Chinstrap       0.50      0.38      0.43        16\n      Gentoo       0.85      1.00      0.92        22\n\n    accuracy                           0.75        69\n   macro avg       0.71      0.72      0.71        69\nweighted avg       0.73      0.75      0.74        69\n\n\n\nWhat about binary classification?\nSo far, we‚Äôve been performing multi-class classification: There were three possible classes {Adelie, Chinstrap, Gentoo}, and each sample could only be a member of a single class.\nMany classification problems can be expressed as binary classification problems. That just means that there are two classes (and all samples must be one or the other, but not both).\nSome metrics make sense with binary problems, but not with multi-class problems. Let‚Äôs change our dataset to make it into a binary classification problem. To do this, we will simply change classification to answer the question ‚ÄúChinstrap or not?‚Äù. So, our new labels will be {Chinstrap, Other}. To do this, we will make a copy of our dataset and modify the species column to reflect the binary labeling.\n\n# Make a binary dataset by splitting the \"Adelie\" and \"Gentoo\" penguins\n# away from the \"Chinstrap\" penguins.\nbinary_penguins = penguins.copy()\nbinary_penguins.loc[binary_penguins[\"species\"] != \"Chinstrap\", \"species\"] = \"Other\"\n\n# Create new train/test split with the new dataset.  (80%/20% as before)\nb_train_df, b_test_df = train_test_split(\n    binary_penguins, test_size=0.20, random_state=2, stratify=binary_penguins[\"species\"]\n)\n# Fit a linear model to the new dataset\nmodel = LogisticRegression(max_iter=500).fit(\n    X=b_train_df[random_var_cols], y=b_train_df[label_col]\n)\n# And predict on the test set.\npreds = model.predict(b_test_df[random_var_cols])\nground_truth = b_test_df[label_col]\nprint(f\"Acc: {accuracy_score(ground_truth, preds):0.3f}\")\nprint(classification_report(ground_truth, preds))\n\nAcc: 0.783\n              precision    recall  f1-score   support\n\n   Chinstrap       0.33      0.07      0.12        14\n       Other       0.80      0.96      0.88        55\n\n    accuracy                           0.78        69\n   macro avg       0.57      0.52      0.50        69\nweighted avg       0.71      0.78      0.72        69\n\n\n\nWe can see that the binary accuracy is about 78%.\n\nReceiver Operating Characteristic (ROC) Curve and Area Under the ROC Curve (AUC)\nA common way of comparing binary classifier is by visually interpreting a performance curve called the Receiver Operating Characteristic (ROC) curve, or by numerically interpreting the area under the ROC curve (AUC or AUROC).\nTo create an ROC curve, we need to predict the probability that each sample belongs to the ‚Äúpositive‚Äù class. In Scikit-Learn compatible models, you can use the predict_proba() method to do this.\n\nprobas = model.predict_proba(b_test_df[random_var_cols])\n\npredict_proba gives a score for each class. For binary problems, we only need the score for the first class (the ‚Äúpositive‚Äù class). We will select that by slicing off the first column from all the rows in probas:\n\nprobas_pos = probas[:, 0]\n\nWe can make a binary (1,0) ground truth by comparing the labels with the first class in our model (which we will consider the ‚Äúpositive‚Äù class):\n\nground_truth = b_test_df[label_col] == model.classes_[0]\n\nThe ROC curve plots the True-Positive Rate (tpr) against the False-Positive Rate (fpr) given all possible thresholds (from 0.0 to 1.0). The roc_auc_score() function from Scikit-Learn can compute the tpr and fpr scores for us, given the ground truth and predicted probabilities:\n\nfpr, tpr, _ = roc_curve(ground_truth, probas_pos)\n\nAnd the roc_auc_score() will calculate the area under the ROC curve, given the same information:\n\nauc = roc_auc_score(ground_truth, probas_pos)\n\nNow, let‚Äôs plot the ROC curve and display the AUC in the legend using Matplotlib:\n\nplt.plot(fpr, tpr, label=f\"auc={auc:0.3f}\")\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.legend(loc=4)\nplt.show()\n\n\n\n\n\n\n\n\nSince we might want to compare multiple models on the same figure, let‚Äôs make a function that will take a dictionary of the form {name: model} of models, the training and testing samples and labels, and plot the ROC curve for all models:\n\n# plots multiple models on the same ROC plot and compare them visually:\ndef multi_auc_comparison(models, X_train, y_train, X_test, y_test):\n    for name in models:\n        model = models[name]\n        model.fit(X_train, y_train)\n        probas = model.predict_proba(X_test)[:, 0]\n        fpr, tpr, _ = roc_curve(y_test, probas)\n        auc = roc_auc_score(y_test, probas)\n        plt.plot(fpr, tpr, label=f\"{name} AUC: {auc:0.3f}\")\n        plt.xlabel(\"fpr\")\n        plt.ylabel(\"tpr\")\n        plt.title(\"ROC Curve Comparision\")\n    plt.legend(loc=4)\n    plt.show()\n\nLet‚Äôs see it in action by comparing the linear logistic regression model to the random forest model:\n\nmulti_auc_comparison(\n    {\n        \"LR\": LogisticRegression(max_iter=500),\n        \"RF\": RandomForestClassifier(random_state=1),\n    },\n    b_train_df[random_var_cols],\n    b_train_df[label_col],\n    b_test_df[random_var_cols],\n    ground_truth,\n)\n\n\n\n\n\n\n\n\nGenerally, the higher AUC is better, but we can see from the ROC curve plot that there is some tradeoff in the performance characteristics (tradeoff between false positives and false negatives)."
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI25.html#working-with-categorical-features",
    "href": "python_intermediate_classification_complete_ASRI25.html#working-with-categorical-features",
    "title": "Classification in Python (Intermediate)",
    "section": "Working with categorical features",
    "text": "Working with categorical features\nSo far, we only used numeric features for our predictors. But, the Palmer Penguins dataset also contains some categorical features:\n\nisland - three levels: [‚ÄòBiscoe‚Äô, ‚ÄòDream‚Äô, ‚ÄòTorgersen‚Äô]\nsex - two levels [‚ÄòMale‚Äô, ‚ÄòFemale‚Äô]\n\nBinary variables (with only two levels) can be re-encoded as 0 and 1 and used essentially the same as a continuous numeric variable.\nVariables with more than two levels require a little more thought. You could encode them using different numeric levels (e.g.¬†{-1, 0, 1}), but this might not always work well. A common approach to multi-level categorical variables is to one-hot encode them.\nOne-hot encoding is an encoding technique in which a variable with \\(N\\) levels is split into \\(N\\) new pseudo-variables where each is a binary variable encoded as 1 or 0.\nLet‚Äôs see how our island variable might look if it were one-hot encoded:\nBefore\nspecies island    bill_length_mm bill_depth_mm ...\nAdelie  Biscoe    38.8           17.2          ...\nAdelie  Torgersen 40.3           18.0          ...\nAdelie  Torgersen 39.1           18.7          ...\nAdelie  Biscoe    37.8           18.3          ...\nAdelie  Dream     39.5           17.8          ...\nAdelie  Biscoe    38.2           18.1          ...\nAdelie  Torgersen 36.7           19.3          ...\nAdelie  Dream     37.2           18.1          ...\nAfter\nspecies island_Biscoe island_Dream island_Torgersen bill_length_mm bill_depth_mm ...\nAdelie  1             0            0                38.8           17.2          ...\nAdelie  0             0            1                40.3           18.0          ...\nAdelie  0             0            1                39.1           18.7          ...\nAdelie  1             0            0                37.8           18.3          ...\nAdelie  0             1            0                39.5           17.8          ...\nAdelie  1             0            0                38.2           18.1          ...\nAdelie  0             0            1                36.7           19.3          ...\nAdelie  0             1            0                37.2           18.1          ...\nHeres the code:\nPandas can do this in a dataframe by using the get_dummies() method. You provide a prefix (like \"island\") and the existing levels are used to complete the new column names.\nBy default, the values will be Boolean (True, False), but we can use dtype=int to make them integers. (‚ÑπÔ∏è You don‚Äôt have to do this - the Boolean values will convert automatically when needed. We do it here just to be explicit about how the categories are becoming numbers.)\n\npenguins_encoded = pd.get_dummies(\n    penguins, columns=[\"island\"], prefix=\"island\", dtype=int\n)\npenguins_encoded.head()\n\n\n    \n\n\n\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nisland_Biscoe\nisland_Dream\nisland_Torgersen\n\n\n\n\n0\nAdelie\n39.1\n18.7\n181.0\n3750.0\nMale\n0\n0\n1\n\n\n1\nAdelie\n39.5\n17.4\n186.0\n3800.0\nFemale\n0\n0\n1\n\n\n2\nAdelie\n40.3\n18.0\n195.0\n3250.0\nFemale\n0\n0\n1\n\n\n4\nAdelie\n36.7\n19.3\n193.0\n3450.0\nFemale\n0\n0\n1\n\n\n5\nAdelie\n39.3\n20.6\n190.0\n3650.0\nMale\n0\n0\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nAs you can see, the island column is gone and replaced with three binary columns. Let‚Äôs query some random rows to see more than just Torgersen island:\n\npenguins_encoded.sample(n=5, random_state=0)\n\n\n    \n\n\n\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nisland_Biscoe\nisland_Dream\nisland_Torgersen\n\n\n\n\n93\nAdelie\n39.6\n18.1\n186.0\n4450.0\nMale\n0\n1\n0\n\n\n281\nGentoo\n46.2\n14.9\n221.0\n5300.0\nMale\n1\n0\n0\n\n\n133\nAdelie\n37.5\n18.5\n199.0\n4475.0\nMale\n0\n1\n0\n\n\n280\nGentoo\n45.3\n13.8\n208.0\n4200.0\nFemale\n1\n0\n0\n\n\n7\nAdelie\n39.2\n19.6\n195.0\n4675.0\nMale\n0\n0\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nThere‚Äôs More Than One Way to Do It\nYou can also use the OneHotEncoder from Scikit-Learn to encode a single variable. It is not as simple as the Pandas method shown above when you have categorical and numeric values in a dataframe, but it works great when you need to one-hot encode your output label. (‚ÑπÔ∏è Some models require that categorical outputs are one-hot encoded. Scikit-Learn models usually don‚Äôt require this.)\nHere‚Äôs how it would look to encode the island column and print five rows.\n\nencoded_island = (\n    OneHotEncoder().fit_transform(penguins[[\"island\"]]).toarray()\n)  # NOTE: the extra [] is necessary to get the correct shape for the single columns we are selecting.\n\n# The following lines are all related to printing five example rows.  The line above did all the hard work.\nnp.random.seed(0)\nidx = np.random.permutation(np.arange(len(encoded_island)))\nprint(encoded_island[idx[:5]])\n\n[[0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]]\n\n\nThere are several also other approaches to encoding categorical values.\nYou can learn a lot more here: https://www.kaggle.com/code/arashnic/an-overview-of-categorical-encoding-methods"
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI25.html#thank-you",
    "href": "python_intermediate_classification_complete_ASRI25.html#thank-you",
    "title": "Classification in Python (Intermediate)",
    "section": "Thank You!",
    "text": "Thank You!\nThis notebook in tutorial and completed form is available at:\nhttps://jcausey-astate.github.io/ASRI-2025/"
  },
  {
    "objectID": "notebooks/python_intermediate_clustering_complete_ASRI25.html#asri-2025",
    "href": "notebooks/python_intermediate_clustering_complete_ASRI25.html#asri-2025",
    "title": "Clustering in Python (Intermediate)",
    "section": "ASRI 2025",
    "text": "ASRI 2025\n\n\n\nClassification in Python (Intermediate)\n\n\nThis notebook shows some introductory examples from the ‚ÄúClustering in Python‚Äù workshop session.\nThe notebook uses the following modules:\n\nmatplotlib : Provides basic graphing/charting.\nnumpy : Allows matrix and vector/array math.\npandas : Provides DataFrame functionality.\nscipy : SciPy provides algorithms for optimization, algebraic equations, statistics and many other classes of problems. We will use it for building dendrograms.\nseaborn : Works with matplotlib to provide nicer graphs.\nsklearn : Scikit-Learn provides machine learning and data manipulation tools.\n\nWe will rely heavily on the Scikit-Learn library for models, metrics, and experimental design tools. See the full documentation for this fantastic library at https://scikit-learn.org.\n\nClustering is an unsupervised learning technique for exploring relationships between the random variables in a dataset. We use a clustering analysis to try to identify groups or similar objects in datasets with two or more random variables.\nIn this tutorial, we will look at some commonly-used clustering techniques:\n\nk-Means Clustering\n\nProbably the most common clustering technique.\nGood for finding clusters that look like ‚Äúblobs‚Äù when visualized.\nYou need to know the number of clusters in advance.\n\nMean Shift Clustering\n\nDoesn‚Äôt require assumptions about the number of clusters.\nSomewhat robust when clusters are not simple ‚Äúblobs‚Äù.\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nClusters areas of similar density, regardless of cluster ‚Äúshape‚Äù.\nWorks for situations where clusters don‚Äôt look like separate ‚Äúblobs‚Äù.\nYou don‚Äôt need to know the number of clusters in advance.\n\nAgglomerative Hierarchical Clustering\n\nEstablishes relationships at all levels of the comparison metric between all samples.\nCan be visualized as a dendrogram.\nAllows you to determine the ‚Äúright‚Äù number of clusters by examining the dendrogram.\n\nGaussian Mixture Model\n\nAssumes the data can be represented as some number of multi-dimensional Gaussian distributions.\nWorks well when the data are ‚Äúblob‚Äù shaped, even when they overlap ‚Äì especially if the density of the blobs differ.\nIdeally, you need to know how many ‚Äúblobs‚Äù to expect.\n\n\n‚ÑπÔ∏è More information about these techniques and many others is available at https://scikit-learn.org/stable/modules/clustering.html."
  },
  {
    "objectID": "notebooks/python_intermediate_clustering_complete_ASRI25.html#lets-see-some-code",
    "href": "notebooks/python_intermediate_clustering_complete_ASRI25.html#lets-see-some-code",
    "title": "Clustering in Python (Intermediate)",
    "section": "Let‚Äôs see some code",
    "text": "Let‚Äôs see some code\nFirst, we have to import the modules, objects, and functions we will be using in this tutorial:\n\n# Ensure that Scikit Learn is recent enough to include HDBSCAN:\n!pip install \"scikit-learn&gt;=1.3\"\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn import cluster\nfrom sklearn import datasets\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import HDBSCAN\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MeanShift\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "notebooks/python_intermediate_clustering_complete_ASRI25.html#comparing-clustering-techniques",
    "href": "notebooks/python_intermediate_clustering_complete_ASRI25.html#comparing-clustering-techniques",
    "title": "Clustering in Python (Intermediate)",
    "section": "Comparing clustering techniques",
    "text": "Comparing clustering techniques\nüí° There is no single perfect clustering algorithm. Different algorithms make different assumptions with regards to what you already know about your dataset.\nFor that reason, we will use a few different datasets to illustrate the strengths and weaknesses of each clustering algorithm.\nHere are the datasets we will use:\n\nPalmer Penguins : This real-world dataset contains measurements of the bill length versus the flipper length of three species of penguins at the Palmer Antarctic research station. https://allisonhorst.github.io/palmerpenguins/articles/intro.html\nMoons : This simulated dataset consists of two crescent ‚Äúmoon‚Äù shapes, with one inverted so that there is no way to linearly separate the two groups, but they are visually separate.\nCircles : This simulated dataset consists of two concentric rings, representing noisy values that are in one of two groups, assigned by the radius of the ring in which the sample is located.\nDensity Blobs : This simulated dataset will consist of three ‚Äúblobs‚Äù with widely varying density and size characteristics. The blobs overlap so that they are not perfectly visually separable.\n\nThe following code block creates the simulated datasets and loads the Palmer Penguins dataset and selects the two predictors we will use for this tutorial.\n\nn_samples = 500\n\n# penguins : this is a real dataset, so we will load it and select a useful set of\n#            features.  We will also store the labels.\npenguins = sns.load_dataset(\"penguins\").dropna(\n    axis=0, subset=[\"bill_length_mm\", \"flipper_length_mm\"]\n)\npenguins_encoder = LabelEncoder().fit(penguins[\"species\"].values)\npenguins_classes = penguins_encoder.classes_\npenguins_labels = penguins_encoder.transform(penguins[\"species\"].values)\npenguins = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].values\npenguins = StandardScaler().fit_transform(penguins)\n\n# moons : two crescent \"moon\" shapes, with one inverted so that there is no way to\n# linearly separate the two groups, but they are visually separate.\nmoons, moons_labels = datasets.make_moons(n_samples=n_samples, noise=0.065)\nmoons = StandardScaler().fit_transform(moons)\n\n# circles : two concentric rings, representing noisy values that are in one of two\n# groups, assigned by the radius of the ring in which the sample is located.\ncircles, circles_labels = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.065\n)\ncircles = StandardScaler().fit_transform(circles)\n\n# density blobs : three \"blobs\" with widely varying density and size characteristics.\n# The blobs overlap so that they are not perfectly visually separable.\ndensity_blobs, density_blobs_labels = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.9, 0.5, 2.7], random_state=4\n)\ndensity_blobs = StandardScaler().fit_transform(density_blobs)\n\nüí° To make understanding the data easier, we need a way to visualize the datasets, plus give hints about what the ‚Äúexpected‚Äù labeling would be as well as what a clustering technique assigns as labels.\nWe will create a function that can take a list of datasets with corresponding names, labels, and styles. The function will use the Seaborn scatterplot() function to plot the datasets in a grid.\n\ndef plot_all(data_list, names=None, label_list=None, style_list=None):\n    \"\"\"\n    Plot scatterplots for all the 2-D datasets in `data_list`.\n    `names` provide a title for each plot\n    `label_list` is a list of label assignments for the samples in the data list.\n    `style_list` is a list of marker shape assignments for the samples in the data list.\n    \"\"\"\n    nrows = int(round(len(data_list) / 2, 0))\n    fig, axs = plt.subplots(nrows=nrows, ncols=2, figsize=(12, 10))\n    for r in range(nrows):\n        for c in range(2):\n            hue = label_list[r * 2 + c] if label_list is not None else None\n            style = style_list[r * 2 + c] if style_list is not None else None\n            title = names[r * 2 + c] if names is not None else None\n            sns.scatterplot(\n                x=data_list[r * 2 + c][:, 0],\n                y=data_list[r * 2 + c][:, 1],\n                hue=hue,\n                style=style,\n                palette=sns.color_palette(n_colors=len(set(hue))),\n                legend=False,\n                ax=axs[r, c],\n            ).set(title=title)\n    plt.subplots_adjust(wspace=0.50)\n    plt.show()\n\nNow, let‚Äôs use the plot_all() function to plot our four datasets. We will use the ‚Äúreal‚Äù labels to color the points so that you can see what the ‚Äúideal‚Äù labeling would be.\n\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\"Penguins\", \"Moons\", \"Circles\", \"Density Blobs\"],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nüí° Keep in mind that in many real-world clustering applications, we will not know what the ‚Äúcorrect‚Äù or ‚Äúideal‚Äù labels are.\nüí° That is the ‚Äúmain idea‚Äù of clustering: It is a technique for suggesting a possible labeling for a dataset by examining relationships between the samples.\n\nk-Means\nLet‚Äôs start with k-Means clustering. k-Means is probably the most well-known clustering technique because it is simple, intuitive to understand, and it works well for clustering lots of datasets where we can visually identify ‚Äúgroups‚Äù in a scatterplot.\nk-Means works by first choosing a value \\(k\\) that represents the desired number of clusters. You need to already know something about the dataset to correctly choose \\(k\\) (for example, you may visualize it first and choose the number that seem right from looking at the scatterplot). There are also some techniques for choosing \\(k\\) if you have no idea of what to try. (See: https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb)\nLet‚Äôs see how k-Means would label each of our test datasets. We ‚Äúknow‚Äù from looking at the scatterplots that there are three groups in the ‚Äúpenguins‚Äù and ‚Äúdensity blobs‚Äù datasets, and there are two groups in the ‚Äúmoons‚Äù and ‚Äúcircles‚Äù datasets. So, we will choose \\(k\\) for each dataset to match our expectation.\n\nkmeans_2 = KMeans(n_clusters=2, random_state=1)  # k-Means with 2 clusters\nkmeans_3 = KMeans(n_clusters=3, random_state=1)  # k-Means with 3 clusters\n\n# We apply k-Means to each dataset with the `fit_transform()` method.  We \"believe\"\n# that \"moons\" and \"circles\" should have k=2 and \"penguins\" and \"density_blobs\"\n# should have k=3.\n#\npenguins_kmeans = kmeans_3.fit_predict(penguins)\nmoons_kmeans = kmeans_2.fit_predict(moons)\ncircles_kmeans = kmeans_2.fit_predict(circles)\ndensity_blobs_kmeans = kmeans_3.fit_predict(density_blobs)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"k-Means Penguins (k=3)\",\n        \"k-Means Moons (k=2)\",\n        \"k-Means Circles (k=2)\",\n        \"k-Means Density Blobs (k=3)\",\n    ],\n    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î We can see that k-Means does a reasonable job of finding the three groups in the ‚Äúpenguins‚Äù dataset. One of those groups (the Chinstrap penguins) is difficult to separate even with more complicated methods. The method does a good job of separating the two ‚Äúeasier‚Äù groups (Adelie and Gentoo).\n‚ùì What if we chose \\(k\\) incorrectly? Let‚Äôs see by first assuming \\(k=2\\) for all four datasets:\n\n# What if we believe there are 2 clusters in each of the datasets?\n#\nkmeans_2 = KMeans(n_clusters=2, random_state=1)  # k-Means with 2 clusters\n\n# We apply k-Means to each dataset with the `fit_transform()` method.\n#\npenguins_kmeans = kmeans_2.fit_predict(penguins)\nmoons_kmeans = kmeans_2.fit_predict(moons)\ncircles_kmeans = kmeans_2.fit_predict(circles)\ndensity_blobs_kmeans = kmeans_2.fit_predict(density_blobs)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"k-Means Penguins (k=2)\",\n        \"k-Means Moons (k=2)\",\n        \"k-Means Circles (k=2)\",\n        \"k-Means Density Blobs (k=2)\",\n    ],\n    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\n‚ùì What do you think about these clusters?\nNow, let‚Äôs incorrectly assume \\(k=4\\) for all four datasets:\n\n# What if we believe there are 4 clusters in all of the datasets?\n#\nkmeans_4 = KMeans(n_clusters=4, random_state=1)  # k-Means with 4 clusters\n\n# We apply k-Means to each dataset with the `fit_transform()` method.\n#\npenguins_kmeans = kmeans_4.fit_predict(penguins)\nmoons_kmeans = kmeans_4.fit_predict(moons)\ncircles_kmeans = kmeans_4.fit_predict(circles)\ndensity_blobs_kmeans = kmeans_4.fit_predict(density_blobs)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Moons (k=4)\",\n        \"k-Means Circles (k=4)\",\n        \"k-Means Density Blobs (k=4)\",\n    ],\n    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\n‚ÑπÔ∏è With k-Means, the clustering is stochastic, meaning that it will be different each time you run it (unless you set the random_state to make it reproducible).\nLet‚Äôs see that in action:\n\n# What if we believe there are 4 clusters in all of the datasets?\n#\nkmeans_4 = KMeans(n_clusters=4)  # k-Means with 4 clusters, no fixed random state.\n\n# We apply k-Means to each dataset with the `fit_transform()` method.\n#\npenguins_kmeans_1 = kmeans_4.fit_predict(penguins)\npenguins_kmeans_2 = kmeans_4.fit_predict(penguins)\npenguins_kmeans_3 = kmeans_4.fit_predict(penguins)\npenguins_kmeans_4 = kmeans_4.fit_predict(penguins)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, penguins, penguins, penguins],\n    [\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Penguins (k=4)\",\n    ],\n    [penguins_kmeans_1, penguins_kmeans_2, penguins_kmeans_3, penguins_kmeans_4],\n    [penguins_labels, penguins_labels, penguins_labels, penguins_labels],\n)\n\n\n\n\n\n\n\n\nüí° Keep in mind when using k-Means: A single run does not give you any indication confidence that the labeling was stable. (A stable labeling is an indication that the labeling is more likely ‚Äúcorrect‚Äù with respect to some real-world relationship.)\nYou can run the clustering several times and compare the stability of the cluster assignments to get an idea of whether the algorithms is seeing ‚Äúreal‚Äù groups or just randomly assigning them based on the initial seed points chosen.\nFor more information on measuring the stability of a clustering assignment, see the following article:\nhttps://amueller.github.io/aml/04-model-evaluation/17-cluster-evaluation.html\n\n\nMean-Shift Clustering\nMean-Shift clustering provides a powerful technique for clustering a dataset when we don‚Äôt already know the number of clusters.\n\n# For each dataset, we need to estimate the bandwidth parameter, then create the\n# MeanShift object.  To automate this, we will wrap the two steps up into a function.\ndef build_MeanShift(X, quantile=0.25):\n    bandwidth = cluster.estimate_bandwidth(X, quantile=quantile)\n    return MeanShift(bandwidth=bandwidth)\n\n\n# Now build a MeanShift clusterer for each dataset and use it to fit/predict.\npenguins_ms = build_MeanShift(penguins).fit_predict(penguins)\nmoons_ms = build_MeanShift(moons).fit_predict(moons)\ncircles_ms = build_MeanShift(circles, quantile=0.2).fit_predict(circles)\ndensity_blobs_ms = build_MeanShift(density_blobs, quantile=0.1).fit_predict(\n    density_blobs\n)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"Mean Shift Penguins\",\n        \"Mean Shift Moons\",\n        \"Mean Shift Circles\",\n        \"Mean Shift Density Blobs\",\n    ],\n    [penguins_ms, moons_ms, circles_ms, density_blobs_ms],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î Mean-Shift did a very good job on the Penguins dataset, without needing to know that there were three groups! It didn‚Äôt do so well with the other datasets though. Circles and Moons are not ‚Äúblob-like‚Äù enough for this technique to work well. In the density blobs dataset, it did (sort of) find the small dense blob‚Ä¶ But at the expense of finding many more small clusters that don‚Äôt really exist.\n‚ÑπÔ∏è The bandwidth parameter is the most important tuning parameter for this algorithm, and it can be tricky to get just right. The estimate_bandwidth() function is provided to help find a good bandwidth value, but it also has a tuning parameter quantile that can be a bit finicky. Some trial-and-error may be required to get results that seem correct.\n\n\nDBSCAN\nThe DBSCAN technique looks for regions of similar density, and assumes that those regions represent groups of samples. It is also capable of automatically identifying outliers (or ‚Äúnoise points‚Äù) that may not belong to any group.\n\n# Like MeanShift, there is an important tuning parameter ('eps') for DBSCAN.\n# Use DBSCAN to cluster each dataset:\npenguins_DBSCAN = DBSCAN(eps=0.25).fit_predict(penguins)\nmoons_DBSCAN = DBSCAN(eps=0.3).fit_predict(moons)\ncircles_DBSCAN = DBSCAN(eps=0.28).fit_predict(circles)\ndensity_blobs_DBSCAN = DBSCAN(eps=0.1).fit_predict(density_blobs)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"DBSCAN Penguins\",\n        \"DBSCAN Moons\",\n        \"DBSCAN Circles\",\n        \"DBSCAN Density Blobs\",\n    ],\n    [penguins_DBSCAN, moons_DBSCAN, circles_DBSCAN, density_blobs_DBSCAN],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î We can see that DBSCAN did a very good job on the ‚Äústrange‚Äù datasets (moons and circles). It also did a decent job with the Penguins dataset. The performance on the density blobs data is interesting: It identified the small dense cluster, but found spurious extra clusters as well. It also failed to see the two larger clusters as separate objects.\n‚ÑπÔ∏è DBSCAN requires careful tuning of the eps parameter, which is related to the expected density of clusters. When there is a wide variation in the density of ‚Äúreal‚Äù clusters, DBSCAN can fail to perform well (as it did here). However, if there is a sparse background of noisy points with some clusters of similar density, DBSCAN will find those clusters even if they are not ‚Äúblob-shaped‚Äù.\n\n\nHDBSCAN\nThe HDBSCAN technique performs DBSCAN over varying epsilon (eps) values and integrates the result to find a clustering that gives the best stability over epsilon. This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN), and be more robust to parameter selection.\n\n# HDBSCAN isn't as sensitive to tuning parameters as DBSCAN, although tuning the \n# `min_cluster_size` can be helpful if you have an idea of how many values should\n# be contained in even the smallest cluster.\n# Use HDBSCAN to cluster each dataset:\npenguins_HDBSCAN = HDBSCAN().fit_predict(penguins)\nmoons_HDBSCAN = HDBSCAN().fit_predict(moons)\ncircles_HDBSCAN = HDBSCAN().fit_predict(circles)\ndensity_blobs_HDBSCAN = HDBSCAN().fit_predict(density_blobs)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"HDBSCAN Penguins\",\n        \"HDBSCAN Moons\",\n        \"HDBSCAN Circles\",\n        \"HDBSCAN Density Blobs\",\n    ],\n    [penguins_HDBSCAN, moons_HDBSCAN, circles_HDBSCAN, density_blobs_HDBSCAN],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î HDBSCAN with defaults for all parameters did a pretty good job. It was really good at the circles and moons, but introduced some spurious extra clusters in penguins and density blobs.\n\n\nAgglomerative Hierarchical Clustering\n\n# Use agglomerative clustering to cluster each dataset:\npenguins_ag = AgglomerativeClustering(n_clusters=3, linkage=\"ward\").fit_predict(\n    penguins\n)\nmoons_ag = AgglomerativeClustering(n_clusters=2, linkage=\"single\").fit_predict(moons)\ncircles_ag = AgglomerativeClustering(n_clusters=2, linkage=\"single\").fit_predict(\n    circles\n)\ndensity_blobs_ag = AgglomerativeClustering(n_clusters=3, linkage=\"ward\").fit_predict(\n    density_blobs\n)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"Hierarchical Penguins\",\n        \"Hierarchical Moons\",\n        \"Hierarchical Circles\",\n        \"Hierarchical Density Blobs\",\n    ],\n    [penguins_ag, moons_ag, circles_ag, density_blobs_ag],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î The hierarchical clustering technique did a pretty good job on all the datasets, but we had to do some tuning. Specifically, the choice of linkage can make a big difference, as can the choice of metric (which we did not tune here). We also provided the model with our desired number of clusters. Agglomerative Clustering will work without the number of clusters provided, but it might not produce the desired result. If you know the expected number of clusters, it is very helpful to provide it.\n‚ÑπÔ∏è Another thing you can do with hierarchical clustering is to visualize the relationships between the samples as a dendrogram. A dendrogram is a tree-like representation that shows every possible clustering from a single group to \\(N\\) groups (where \\(N\\) is the total number of samples). The height of the vertical lines represent changes in similarity between splits.\nYou can use this visualization to get an idea of how many clusters should exist in the dataset. This can be used to determine \\(k\\) before performing k-Means clustering, or for other models that require knowing the number of groups beforehand.\nüíÅ The dendrogram visualization is available from SciPy. (See: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram)\nTo see it in action, we will generate dendrogram for the ‚Äúpenguins‚Äù dataset:\n\nlinkage_matrix = linkage(penguins, \"ward\")\nplt.figure(figsize=(10, 9))\nplt.title(\"Hierarchical Penguins Dendrogram\")\nplt.ylabel(\"distance (Ward)\")\ndendrogram(linkage_matrix, no_labels=True, color_threshold=10)\nplt.show()\n\n\n\n\n\n\n\n\nBy examining the vertical lines, you can determine the ‚Äúbest‚Äù number of clusters by choosing where to ‚Äúcut‚Äù the graph with an imaginary horizontal line.\n\n\nGaussian Mixture Model\nThe Gaussian Mixture Model (GMM) works by assuming that clusters are multi-dimensional Gaussian distributions (which look like roughly ‚Äúcircular‚Äù blobs that are denser in the middle). It is quite flexible if you have ‚Äúblob-like‚Äù clusters, and GMM-like clusters occur frequently in natural datasets.\n\nfrom sklearn.mixture import GaussianMixture\n\n# Use a GMM to cluster each dataset.  We assume that penguins and\n# density blobs have three clusters, and that moons and circles have two.\n#\npenguins_ag = GaussianMixture(n_components=3).fit_predict(penguins)\nmoons_ag = GaussianMixture(n_components=2).fit_predict(moons)\ncircles_ag = GaussianMixture(n_components=2).fit_predict(circles)\ndensity_blobs_ag = GaussianMixture(n_components=3).fit_predict(density_blobs)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"GMM Penguins\",\n        \"GMM Moons\",\n        \"GMM Circles\",\n        \"GMM Density Blobs\",\n    ],\n    [penguins_ag, moons_ag, circles_ag, density_blobs_ag],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î The GMM did a very good job on the ‚Äúblob-like‚Äù datasets ‚Äúpenguins‚Äù and ‚Äúdensity blobs‚Äù. In fact, it probably did best on density blobs (versus the other methods we tried).\nHowever, it does not do as well with the datasets whose values aren‚Äôt ‚Äúblob-like‚Äù in shape."
  },
  {
    "objectID": "notebooks/python_intermediate_clustering_complete_ASRI25.html#thank-you",
    "href": "notebooks/python_intermediate_clustering_complete_ASRI25.html#thank-you",
    "title": "Clustering in Python (Intermediate)",
    "section": "Thank You!",
    "text": "Thank You!\nThis notebook in tutorial and completed form is available at:\nhttps://jcausey-astate.github.io/ASRI-2025/"
  },
  {
    "objectID": "index.html#june-05-regression-techniques-in-python-intermediate",
    "href": "index.html#june-05-regression-techniques-in-python-intermediate",
    "title": "Notebooks for ASRI-2025 Summer Workshops",
    "section": "June 05: Regression Techniques in Python (Intermediate)",
    "text": "June 05: Regression Techniques in Python (Intermediate)\n\n\nWorkshop version (with open exercises to complete)\nhttps://colab.research.google.com/github/jcausey-astate/ASRI-2025/blob/main/python_intermediate_regression_ASRI25.ipynb\n\n\nCompleted version for reference\nhttps://colab.research.google.com/github/jcausey-astate/ASRI-2025/blob/main/python_intermediate_regression_complete_ASRI25.ipynb\n\n\nNon-interactive web version"
  },
  {
    "objectID": "index.html#june-06-classification-techniques-in-python-intermediate",
    "href": "index.html#june-06-classification-techniques-in-python-intermediate",
    "title": "Notebooks for ASRI-2025 Summer Workshops",
    "section": "June 06: Classification Techniques in Python (Intermediate)",
    "text": "June 06: Classification Techniques in Python (Intermediate)"
  },
  {
    "objectID": "index.html#june-10-clustering-techniques-in-python-intermediate",
    "href": "index.html#june-10-clustering-techniques-in-python-intermediate",
    "title": "Notebooks for ASRI-2025 Summer Workshops",
    "section": "June 10: Clustering Techniques in Python (Intermediate)",
    "text": "June 10: Clustering Techniques in Python (Intermediate)\n\n\n\n\n2025 Jason L. Causey, Department of Computer Science, Arkansas State University. ORCiD: 0000-0002-3985-2919"
  },
  {
    "objectID": "notebooks/python_intermediate_classification_complete_ASRI25.html#asri-2025",
    "href": "notebooks/python_intermediate_classification_complete_ASRI25.html#asri-2025",
    "title": "Classification in Python (Intermediate)",
    "section": "ASRI 2025",
    "text": "ASRI 2025\n\n\n\nClassification in Python (Intermediate)\n\n\n\n\n\nThe Palmer Archipelago penguins. Artwork by @allison_horst.\n\n\nThe notebook uses the following modules:\n\nmatplotlib : Provides basic graphing/charting.\nnumpy : Allows matrix and vector/array math.\npandas : Provides DataFrame functionality.\nseaborn : Works with matplotlib to provide nicer graphs.\nsklearn : Scikit-Learn provides machine learning and data manipulation tools.\n\nWe will rely heavily on the Scikit-Learn library for models, metrics, and experimental design tools. See the full documentation for this fantastic library at https://scikit-learn.org."
  },
  {
    "objectID": "notebooks/python_intermediate_classification_complete_ASRI25.html#first-some-terms-and-definitions",
    "href": "notebooks/python_intermediate_classification_complete_ASRI25.html#first-some-terms-and-definitions",
    "title": "Classification in Python (Intermediate)",
    "section": "First, some terms and definitions:",
    "text": "First, some terms and definitions:\nClassification is the process of determining a categorical label given the random variables for a given sample.\nCategorical values are allowed to take on only a finite (usually small) set of values. Categorical variables are usually non-numeric, but are sometimes encoded as numbers. Sometimes we refer to values of this type as labels, factors, or classes.\nA sample consists of all of the experimental information gathered for one item in the dataset. Sometimes a sample is called an object or item. Usually samples are arranged as rows in tabular datasets (CSV files, Excel spreadsheets, or similar).\nA random variable, sometimes called an input variable, measurement, or feature, is the recorded value for some property of the sample that was measured in the experiment, e.g.¬†‚Äúheight‚Äù, ‚Äúage‚Äù, ‚Äúflower color‚Äù, etc.\n\nYou have a classification problem if the dependent variable (output value) you are trying to predict is categorical.\nWe will focus first on classification problems where the random variables are continuous.\nContinuous values are numeric values that are allowed to take on any value within some range.\nAt the end, a section is provided with some tips for working with random variables that are categorical."
  },
  {
    "objectID": "notebooks/python_intermediate_classification_complete_ASRI25.html#lets-see-some-code",
    "href": "notebooks/python_intermediate_classification_complete_ASRI25.html#lets-see-some-code",
    "title": "Classification in Python (Intermediate)",
    "section": "Let‚Äôs see some code!",
    "text": "Let‚Äôs see some code!\nFirst, we have to import the modules, objects, and functions we will be using in this tutorial:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nThe Dataset\n‚ÑπÔ∏è The seaborn package has some sample datasets included.\nFor this tutorial, we will use the ‚ÄúPalmer Penguins‚Äù dataset, which is called penguins in the Seaborn index. We can load it with the load_dataset() function. It will load up as a Pandas DataFrame.\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n    \n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThe species column contains the value that we want to predict (it is our label column). Although we could use all the other columns as random variables (predictors), we will only focus on the numeric values for this part of the tutorial.\nIt will make things easier if we create variables to contain the name of the label column and the random variables. These can be used when we interact with Pandas DataFrames to quickly select those columns by name. This way, we don‚Äôt have to type the list of names often, and we don‚Äôt have to create a different data structure that only contains our variables of interest (although you could also do it that way).\n\nlabel_col = \"species\"\nrandom_var_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\",\n]\n\nLet‚Äôs use the info() DataFrame method to see what kinds of values we have, and whether there are any missing values.\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\nNotice that there are some missing values. We care most about the numeric columns for this example, so we want to drop any rows with missing values in those columns.\nThe dropna() method can do this. The subset parameter lets us specify which columns we care about (the random variables we specified earlier). We use axis=0 to indicate that we want to drop rows, not columns.\n\npenguins = penguins.dropna(subset=random_var_cols, axis=0)\n\n\n\nüìä Visualize Early, Visualize Often\nLet‚Äôs take a look at the dataset. We will plot two different ‚Äòviews‚Äô for comparison. the first will compare bill length with bill depth, and the second will compare bill length with flipper length.\nWe can color the datapoints according to species so that we can visually see how separable the different classes might be.\n\n# create a figure and two subplots\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\nplt.suptitle(\"Separating penguin species by bill measurements and/or flipper length.\")\n\n# create first scatterplot using Seaborn\nsns.scatterplot(\n    data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", ax=ax1\n)\nax1.set_title(\"Bill Length vs Bill Depth (mm)\")\n\n# create second scatterplot just like the first, but with different columns\nsns.scatterplot(\n    data=penguins, x=\"bill_length_mm\", y=\"flipper_length_mm\", hue=\"species\", ax=ax2\n)\nax2.set_title(\"Bill Length vs Flipper Length (mm)\")\n\n# adjust spacing between subplots\nplt.subplots_adjust(wspace=0.15)\n\n# show the plots\nplt.show()\n\n\n\n\n\n\n\n\nIf we look at these plots, it seems we can probably do a pretty good job of separating the three classes. We see that you could even get pretty good performance by drawing a few lines to separate the groups (in other words, a simple linear model might work reasonably well).\nTo see what a harder classification problem might look like, let‚Äôs draw another scatterplot where we compare the flipper length and the body mass:\n\nsns.scatterplot(data=penguins, x=\"flipper_length_mm\", y=\"body_mass_g\", hue=\"species\")\nplt.title(\"A more difficult problem...\")\nplt.show()\n\n\n\n\n\n\n\n\nIn this plot, it is very hard to see how we could separate the ‚ÄúAdelie‚Äù group from the ‚ÄúChinstrap‚Äù group. There is even some mixing between the ‚ÄúChinstrap‚Äù and ‚ÄúGentoo‚Äù groups.\n‚ú® Choosing the right random variables for prediction is vital. This is why it is a good idea to get to know your dataset early in the process! Visualize early, visualize often!"
  },
  {
    "objectID": "notebooks/python_intermediate_classification_complete_ASRI25.html#lets-see-how-well-we-can-classify-with-a-linear-model.",
    "href": "notebooks/python_intermediate_classification_complete_ASRI25.html#lets-see-how-well-we-can-classify-with-a-linear-model.",
    "title": "Classification in Python (Intermediate)",
    "section": "Let‚Äôs see how well we can classify with a linear model.",
    "text": "Let‚Äôs see how well we can classify with a linear model.\nFirst, we examine the LogisticRegression model (which is actually a classification model ‚Äì don‚Äôt let the name fool you).\nBased on the graphs we plotted above, let‚Äôs use the bill length and depth as our random variables. (‚ÑπÔ∏è : We could absolutely use all four random variables and it would probably do better, but using just two gives us a chance to discuss the performance with a very simple model.)\n\nrandom_var_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n]\n\nTo quickly determine if it will be suitable to this problem, we can use the cross_val_score() function from Scikit-Learn. This function wraps up a lot of functionality. It will set up a k-fold cross validation experiment (with default of \\(k=5\\), for five-fold CV). It will take the model of your choice and automatically train the model for each training fold, then predict the test cases and score the predictions on the test folds (with the accuracy metric by default).\nThe scores for each fold are returned. We can calculate and report the mean score over all five folds along with the standard deviation of the scores to see whether the model is able to do a good job in general, and how much variation we would expect for different training sets. Models should have high accuracy, and a low standard deviation would indicate that the model generalizes to new data very well. (A high standard deviation would indicate the model is unstable and doesn‚Äôt generalize well.)\n\nscores = cross_val_score(\n    LogisticRegression(max_iter=500), X=penguins[random_var_cols], y=penguins[label_col]\n)\nprint(\n    f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\"\n)  # print mean and standard deviation\n\nmean: 0.962, std: 0.020\n\n\nüéâ Wow! The linear model does a really good job on this problem!\nOK, that isn‚Äôt really surprising since we looked at the data first and we could see that some combinations of our random variables provided good linear separation between the groups. Still, it‚Äôs nice to see our intuition was correct.\nLet‚Äôs take a look at a different kind of model, just for comparison. A Random Forest model is a non-linear model that works well for lots of tasks. Scikit-Learn provides one called RandomForestClassifier.\nLet‚Äôs try it in exactly the same experimental setup we used for the linear model.\n\nscores = cross_val_score(\n    RandomForestClassifier(random_state=1),\n    X=penguins[random_var_cols],\n    y=penguins[label_col],\n)\nprint(f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\")\n\nmean: 0.968, std: 0.017\n\n\nHere, the random forest did about the same as the linear model (especially if we take the standard deviations of scores into account).\nIf we wanted to pick between these two models for this problem, we should probably choose the simpler one ‚Äì the logistic regression model.\nüí° The Principle of Parsimony says that given the choice between multiple models with similar performance, the best choice is usually the simplest model.\n‚ÑπÔ∏è One note:\nWe used random_state=1 to seed the random number generator within the model, causing it to produce identical results if we train it again on the same data. Random forests (as implied by their name) rely on some randomness during training, so you don‚Äôt expect to get the same performance every time. This makes reproducible results difficult.\nüí° By seeding the random state, we ‚Äúlock‚Äù it to a specific outcome (assuming no external changes). This way, others can reproduce our results in the future."
  },
  {
    "objectID": "notebooks/python_intermediate_classification_complete_ASRI25.html#exploring-more-ways-to-characterize-classifier-performance.",
    "href": "notebooks/python_intermediate_classification_complete_ASRI25.html#exploring-more-ways-to-characterize-classifier-performance.",
    "title": "Classification in Python (Intermediate)",
    "section": "Exploring more ways to characterize classifier performance.",
    "text": "Exploring more ways to characterize classifier performance.\n\nüìä Visualize!\nWhen the model is making incorrect predictions, sometimes we want to know which samples the model predicts incorrectly. This can help us diagnose whether the model is doing the best it can, whether the model is doing strange things, or even whether there might be a problem with the dataset itself.\nGenerally, a good starting point to diagnosing the mis-predicted values from a model is for us to visualize them in some way. Since this problem is easy to visualize as a 2-D scatterplot, we will use that as a way to see which samples the model is getting right vs.¬†wrong.\nTo start, let‚Äôs just split the dataset into a simple 80% / 20% train / test split. That means that we will reserve 20% of the samples for the test set, and the other 80% will be used for training. Scikit-Learn has a simple function for doing this (train_test_split()).\nThe function returns a training and testing dataframe (or matrix) given the original full dataset and the fraction you want to hold out for the test set.\n\ntrain_df, test_df = train_test_split(penguins, test_size=0.20, random_state=2)\n\nWe‚Äôll use the LogisticRegression model again, training it on the ‚Äútrain‚Äù partition (using the fit() method). Then, we‚Äôll predict the ‚Äútest‚Äù samples and calculate the (balanced) accuracy score.\n\nmodel = LogisticRegression(max_iter=500).fit(\n    X=train_df[random_var_cols], y=train_df[label_col]\n)\npreds = model.predict(test_df[random_var_cols])\nground_truth = test_df[label_col]\nprint(f\"{balanced_accuracy_score(ground_truth, preds):0.3f}\")\n\n0.932\n\n\nNow, let‚Äôs create the scatterplot that will show which samples were predicted incorrectly.\nWe can use color to indicate correct (green) and incorrect (red) predictions. We will also use different marker shapes to indicate the true class label so that we can see which ones are being predicted incorrectly and get a sense for why.\n\nplt.figure()\nfig = sns.scatterplot(\n    data=test_df,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    hue=(preds == ground_truth),\n    style=list(test_df[label_col].values),\n    markers=[\"o\", \"D\", \"s\"],\n    palette=[\"red\", \"green\"],\n)\nfig.legend()  # weird kludge:  The \"species\" title is shown by default, but just calling `legend()` removes it.  Why? ü§∑\nplt.title(\"Correct (green) and Incorrect (red) predictions.\")\nprint(test_df[label_col].unique())\n\n['Gentoo' 'Adelie' 'Chinstrap']\n\n\n\n\n\n\n\n\n\nThe green dots are samples that were correctly predicted and the red dots are incorrect predictions. There are only four incorrect predictions. Three of those are near the ‚Äúborder‚Äù between the two visual ‚Äúclusters‚Äù. That makes sense ‚Äì class mixing obviously occurs here. The other one is a Gentoo penguin that was incorrectly predicted (probably as a Chinstrap, since there are Chinstrap penguins nearby).\nAt this point, we will make the problem harder. Why? Well, it will be more interesting to explore correct / incorrect predictions if the model is not quite so good.\nThe flipper length and body mass do not combine to give very good separation, so we will choose those as our random variables from this point forward.\nü§î Of course, we would never choose worse predictors in any real analysis, but doing this can be useful as a learning exercise.\n\nrandom_var_cols = [\"flipper_length_mm\", \"body_mass_g\"]\n\nLet‚Äôs see how our new random variables perform with the same linear model as before:\n\nscores = cross_val_score(\n    LogisticRegression(max_iter=500), X=penguins[random_var_cols], y=penguins[label_col]\n)\nprint(f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\")\n\nmean: 0.690, std: 0.061\n\n\nMuch worse performance! üò¶ That‚Äôs bad‚Ä¶ But more interesting for exploring the performance metrics.\n\nscores = cross_val_score(\n    RandomForestClassifier(random_state=1),\n    X=penguins[random_var_cols],\n    y=penguins[label_col],\n)\nprint(f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\")\n\nmean: 0.772, std: 0.041\n\n\nThe random forest did quite a bit better here. We saw that these two variables don‚Äôt provide an obvious path for linear separation, but the random forest is not limited to linear decision boundaries.\nWe will use our simple 80%/20% train/test split from earlier and train the linear model using the new (worse) combination of random variables. First, train the model:\n\nmodel = LogisticRegression(max_iter=500).fit(\n    X=train_df[random_var_cols], y=train_df[label_col]\n)\npreds = model.predict(test_df[random_var_cols])\nground_truth = test_df[label_col]\nprint(f\"{balanced_accuracy_score(ground_truth, preds):0.3f}\")\n\n0.716\n\n\n\n\nConfusion Matrix\nNow, we can use another visualization technique to discuss the performance characteristics. This technique is called a confusion matrix. It shows the number of samples from each true label that were predicted as each possible output label. Seaborn makes very nice confusion matrix plots.\n\ncm = confusion_matrix(ground_truth, preds)\n\n# plot the confusion matrix using seaborn heatmap\nsns.set(font_scale=1.4)  # adjust font size\nlabels = model.classes_\nsns.heatmap(\n    cm, annot=True, fmt=\"g\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels\n)\n\n# add axis labels and title\nplt.xlabel(\"Predicted label\")\nplt.ylabel(\"True label\")\nplt.title(\"Confusion Matrix\")\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\nCorrect predictions appear along the diagonal (upper-left to lower-right). All of the other squares represent incorrect predictions.\nHere, the linear model did very well with predicting Gentoo penguins when it saw a real Gentoo. But it also incorrectly guessed that 3 Chinstraps and 1 Adelie were Gentoos as well (False Positives).\nAs for the Chinstraps, we correctly classified 6 of them, but we incorrectly labeled 7 Chinstraps as Adelie and 3 as Gentoo (False Negatives).\n\n\nNon-visual metrics\nLet‚Äôs look at other classification metrics.\nScikit-Learn provides several metrics appropriate for evaluating classification models. You can see the list at https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics.\nWe will start with the classification_report() function, which combines several popular metrics into a single report.\n\n# We get precision, recall, and f1-score from the classification report.\n# You can also get these individually from functions in sklearn.metrics.\nprint(classification_report(ground_truth, preds))\n\n              precision    recall  f1-score   support\n\n      Adelie       0.77      0.77      0.77        31\n   Chinstrap       0.50      0.38      0.43        16\n      Gentoo       0.85      1.00      0.92        22\n\n    accuracy                           0.75        69\n   macro avg       0.71      0.72      0.71        69\nweighted avg       0.73      0.75      0.74        69\n\n\n\nWhat about binary classification?\nSo far, we‚Äôve been performing multi-class classification: There were three possible classes {Adelie, Chinstrap, Gentoo}, and each sample could only be a member of a single class.\nMany classification problems can be expressed as binary classification problems. That just means that there are two classes (and all samples must be one or the other, but not both).\nSome metrics make sense with binary problems, but not with multi-class problems. Let‚Äôs change our dataset to make it into a binary classification problem. To do this, we will simply change classification to answer the question ‚ÄúChinstrap or not?‚Äù. So, our new labels will be {Chinstrap, Other}. To do this, we will make a copy of our dataset and modify the species column to reflect the binary labeling.\n\n# Make a binary dataset by splitting the \"Adelie\" and \"Gentoo\" penguins\n# away from the \"Chinstrap\" penguins.\nbinary_penguins = penguins.copy()\nbinary_penguins.loc[binary_penguins[\"species\"] != \"Chinstrap\", \"species\"] = \"Other\"\n\n# Create new train/test split with the new dataset.  (80%/20% as before)\nb_train_df, b_test_df = train_test_split(\n    binary_penguins, test_size=0.20, random_state=2, stratify=binary_penguins[\"species\"]\n)\n# Fit a linear model to the new dataset\nmodel = LogisticRegression(max_iter=500).fit(\n    X=b_train_df[random_var_cols], y=b_train_df[label_col]\n)\n# And predict on the test set.\npreds = model.predict(b_test_df[random_var_cols])\nground_truth = b_test_df[label_col]\nprint(f\"Acc: {accuracy_score(ground_truth, preds):0.3f}\")\nprint(classification_report(ground_truth, preds))\n\nAcc: 0.783\n              precision    recall  f1-score   support\n\n   Chinstrap       0.33      0.07      0.12        14\n       Other       0.80      0.96      0.88        55\n\n    accuracy                           0.78        69\n   macro avg       0.57      0.52      0.50        69\nweighted avg       0.71      0.78      0.72        69\n\n\n\nWe can see that the binary accuracy is about 78%.\n\nReceiver Operating Characteristic (ROC) Curve and Area Under the ROC Curve (AUC)\nA common way of comparing binary classifier is by visually interpreting a performance curve called the Receiver Operating Characteristic (ROC) curve, or by numerically interpreting the area under the ROC curve (AUC or AUROC).\nTo create an ROC curve, we need to predict the probability that each sample belongs to the ‚Äúpositive‚Äù class. In Scikit-Learn compatible models, you can use the predict_proba() method to do this.\n\nprobas = model.predict_proba(b_test_df[random_var_cols])\n\npredict_proba gives a score for each class. For binary problems, we only need the score for the first class (the ‚Äúpositive‚Äù class). We will select that by slicing off the first column from all the rows in probas:\n\nprobas_pos = probas[:, 0]\n\nWe can make a binary (1,0) ground truth by comparing the labels with the first class in our model (which we will consider the ‚Äúpositive‚Äù class):\n\nground_truth = b_test_df[label_col] == model.classes_[0]\n\nThe ROC curve plots the True-Positive Rate (tpr) against the False-Positive Rate (fpr) given all possible thresholds (from 0.0 to 1.0). The roc_auc_score() function from Scikit-Learn can compute the tpr and fpr scores for us, given the ground truth and predicted probabilities:\n\nfpr, tpr, _ = roc_curve(ground_truth, probas_pos)\n\nAnd the roc_auc_score() will calculate the area under the ROC curve, given the same information:\n\nauc = roc_auc_score(ground_truth, probas_pos)\n\nNow, let‚Äôs plot the ROC curve and display the AUC in the legend using Matplotlib:\n\nplt.plot(fpr, tpr, label=f\"auc={auc:0.3f}\")\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.legend(loc=4)\nplt.show()\n\n\n\n\n\n\n\n\nSince we might want to compare multiple models on the same figure, let‚Äôs make a function that will take a dictionary of the form {name: model} of models, the training and testing samples and labels, and plot the ROC curve for all models:\n\n# plots multiple models on the same ROC plot and compare them visually:\ndef multi_auc_comparison(models, X_train, y_train, X_test, y_test):\n    for name in models:\n        model = models[name]\n        model.fit(X_train, y_train)\n        probas = model.predict_proba(X_test)[:, 0]\n        fpr, tpr, _ = roc_curve(y_test, probas)\n        auc = roc_auc_score(y_test, probas)\n        plt.plot(fpr, tpr, label=f\"{name} AUC: {auc:0.3f}\")\n        plt.xlabel(\"fpr\")\n        plt.ylabel(\"tpr\")\n        plt.title(\"ROC Curve Comparision\")\n    plt.legend(loc=4)\n    plt.show()\n\nLet‚Äôs see it in action by comparing the linear logistic regression model to the random forest model:\n\nmulti_auc_comparison(\n    {\n        \"LR\": LogisticRegression(max_iter=500),\n        \"RF\": RandomForestClassifier(random_state=1),\n    },\n    b_train_df[random_var_cols],\n    b_train_df[label_col],\n    b_test_df[random_var_cols],\n    ground_truth,\n)\n\n\n\n\n\n\n\n\nGenerally, the higher AUC is better, but we can see from the ROC curve plot that there is some tradeoff in the performance characteristics (tradeoff between false positives and false negatives)."
  },
  {
    "objectID": "notebooks/python_intermediate_classification_complete_ASRI25.html#working-with-categorical-features",
    "href": "notebooks/python_intermediate_classification_complete_ASRI25.html#working-with-categorical-features",
    "title": "Classification in Python (Intermediate)",
    "section": "Working with categorical features",
    "text": "Working with categorical features\nSo far, we only used numeric features for our predictors. But, the Palmer Penguins dataset also contains some categorical features:\n\nisland - three levels: [‚ÄòBiscoe‚Äô, ‚ÄòDream‚Äô, ‚ÄòTorgersen‚Äô]\nsex - two levels [‚ÄòMale‚Äô, ‚ÄòFemale‚Äô]\n\nBinary variables (with only two levels) can be re-encoded as 0 and 1 and used essentially the same as a continuous numeric variable.\nVariables with more than two levels require a little more thought. You could encode them using different numeric levels (e.g.¬†{-1, 0, 1}), but this might not always work well. A common approach to multi-level categorical variables is to one-hot encode them.\nOne-hot encoding is an encoding technique in which a variable with \\(N\\) levels is split into \\(N\\) new pseudo-variables where each is a binary variable encoded as 1 or 0.\nLet‚Äôs see how our island variable might look if it were one-hot encoded:\nBefore\nspecies island    bill_length_mm bill_depth_mm ...\nAdelie  Biscoe    38.8           17.2          ...\nAdelie  Torgersen 40.3           18.0          ...\nAdelie  Torgersen 39.1           18.7          ...\nAdelie  Biscoe    37.8           18.3          ...\nAdelie  Dream     39.5           17.8          ...\nAdelie  Biscoe    38.2           18.1          ...\nAdelie  Torgersen 36.7           19.3          ...\nAdelie  Dream     37.2           18.1          ...\nAfter\nspecies island_Biscoe island_Dream island_Torgersen bill_length_mm bill_depth_mm ...\nAdelie  1             0            0                38.8           17.2          ...\nAdelie  0             0            1                40.3           18.0          ...\nAdelie  0             0            1                39.1           18.7          ...\nAdelie  1             0            0                37.8           18.3          ...\nAdelie  0             1            0                39.5           17.8          ...\nAdelie  1             0            0                38.2           18.1          ...\nAdelie  0             0            1                36.7           19.3          ...\nAdelie  0             1            0                37.2           18.1          ...\nHeres the code:\nPandas can do this in a dataframe by using the get_dummies() method. You provide a prefix (like \"island\") and the existing levels are used to complete the new column names.\nBy default, the values will be Boolean (True, False), but we can use dtype=int to make them integers. (‚ÑπÔ∏è You don‚Äôt have to do this - the Boolean values will convert automatically when needed. We do it here just to be explicit about how the categories are becoming numbers.)\n\npenguins_encoded = pd.get_dummies(\n    penguins, columns=[\"island\"], prefix=\"island\", dtype=int\n)\npenguins_encoded.head()\n\n\n    \n\n\n\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nisland_Biscoe\nisland_Dream\nisland_Torgersen\n\n\n\n\n0\nAdelie\n39.1\n18.7\n181.0\n3750.0\nMale\n0\n0\n1\n\n\n1\nAdelie\n39.5\n17.4\n186.0\n3800.0\nFemale\n0\n0\n1\n\n\n2\nAdelie\n40.3\n18.0\n195.0\n3250.0\nFemale\n0\n0\n1\n\n\n4\nAdelie\n36.7\n19.3\n193.0\n3450.0\nFemale\n0\n0\n1\n\n\n5\nAdelie\n39.3\n20.6\n190.0\n3650.0\nMale\n0\n0\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nAs you can see, the island column is gone and replaced with three binary columns. Let‚Äôs query some random rows to see more than just Torgersen island:\n\npenguins_encoded.sample(n=5, random_state=0)\n\n\n    \n\n\n\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nisland_Biscoe\nisland_Dream\nisland_Torgersen\n\n\n\n\n93\nAdelie\n39.6\n18.1\n186.0\n4450.0\nMale\n0\n1\n0\n\n\n281\nGentoo\n46.2\n14.9\n221.0\n5300.0\nMale\n1\n0\n0\n\n\n133\nAdelie\n37.5\n18.5\n199.0\n4475.0\nMale\n0\n1\n0\n\n\n280\nGentoo\n45.3\n13.8\n208.0\n4200.0\nFemale\n1\n0\n0\n\n\n7\nAdelie\n39.2\n19.6\n195.0\n4675.0\nMale\n0\n0\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nThere‚Äôs More Than One Way to Do It\nYou can also use the OneHotEncoder from Scikit-Learn to encode a single variable. It is not as simple as the Pandas method shown above when you have categorical and numeric values in a dataframe, but it works great when you need to one-hot encode your output label. (‚ÑπÔ∏è Some models require that categorical outputs are one-hot encoded. Scikit-Learn models usually don‚Äôt require this.)\nHere‚Äôs how it would look to encode the island column and print five rows.\n\nencoded_island = (\n    OneHotEncoder().fit_transform(penguins[[\"island\"]]).toarray()\n)  # NOTE: the extra [] is necessary to get the correct shape for the single columns we are selecting.\n\n# The following lines are all related to printing five example rows.  The line above did all the hard work.\nnp.random.seed(0)\nidx = np.random.permutation(np.arange(len(encoded_island)))\nprint(encoded_island[idx[:5]])\n\n[[0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]]\n\n\nThere are several also other approaches to encoding categorical values.\nYou can learn a lot more here: https://www.kaggle.com/code/arashnic/an-overview-of-categorical-encoding-methods"
  },
  {
    "objectID": "notebooks/python_intermediate_classification_complete_ASRI25.html#thank-you",
    "href": "notebooks/python_intermediate_classification_complete_ASRI25.html#thank-you",
    "title": "Classification in Python (Intermediate)",
    "section": "Thank You!",
    "text": "Thank You!\nThis notebook in tutorial and completed form is available at:\nhttps://jcausey-astate.github.io/ASRI-2025/"
  },
  {
    "objectID": "notebooks/python_intermediate_regression_complete_ASRI25.html#asri-2025",
    "href": "notebooks/python_intermediate_regression_complete_ASRI25.html#asri-2025",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "ASRI 2025",
    "text": "ASRI 2025\n\n\n\nRegression in Python (Intermediate)\n\n\nThe notebook uses the following modules:\n\nmatplotlib : Provides basic graphing/charting.\nnumpy : Allows matrix and vector/array math.\npandas : Provides DataFrame functionality.\nseaborn : Works with matplotlib to provide nicer graphs.\nsklearn : Scikit-Learn provides machine learning and data manipulation tools.\n\nWe will rely heavily on the Scikit-Learn library for models, metrics, and experimental design tools. See the full documentation for this fantastic library at https://scikit-learn.org."
  },
  {
    "objectID": "notebooks/python_intermediate_regression_complete_ASRI25.html#first-some-terms-and-definitions",
    "href": "notebooks/python_intermediate_regression_complete_ASRI25.html#first-some-terms-and-definitions",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "First, some terms and definitions:",
    "text": "First, some terms and definitions:\nRegression is the process of predicting a continuous value given the random variables for a given sample.\nContinuous values are numeric values that can take on any value within some range. Examples include height, weight, temperature, price, etc.\nA sample consists of all of the experimental information gathered for one item in the dataset. Sometimes a sample is called an object or item. Usually samples are arranged as rows in tabular datasets (CSV files, Excel spreadsheets, or similar).\nA random variable, sometimes called an input variable, measurement, or feature, is the recorded value for some property of the sample that was measured in the experiment, e.g.¬†‚Äúweight‚Äù, ‚Äúhorsepower‚Äù, ‚Äúnumber of cylinders‚Äù, etc.\n\nYou have a regression problem if the dependent variable (output value) you are trying to predict is continuous.\nWe will focus first on regression problems where the random variables are also continuous.\nAt the end, a section is provided with some tips for working with random variables that are categorical."
  },
  {
    "objectID": "notebooks/python_intermediate_regression_complete_ASRI25.html#lets-see-some-code",
    "href": "notebooks/python_intermediate_regression_complete_ASRI25.html#lets-see-some-code",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "Let‚Äôs see some code!",
    "text": "Let‚Äôs see some code!\nFirst, we have to import the modules, objects, and functions we will be using in this tutorial:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\nThe Dataset\nFor this tutorial, we will use the ‚ÄúAuto MPG‚Äù dataset, which is a classic dataset for regression tasks. It contains information about various automobiles, including their fuel consumption in miles per gallon (mpg, which will be our target variable to predict.\nLet‚Äôs load the dataset and take a look at it:\n\n# Define column names based on the dataset description\ncolumn_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', \n                'acceleration', 'model_year', 'origin', 'car_name']\n\n# Download the dataset from UCI ML Repository if needed.\n! [[ -f auto+mpg.zip ]] || { wget https://archive.ics.uci.edu/static/public/9/auto+mpg.zip && unzip -o auto+mpg.zip && rm Index auto-mpg.data-original; }\n\n# Load the dataset\nauto_mpg = pd.read_csv('auto-mpg.data', sep=r'\\s+', names=column_names)\n\n# Display the first few rows\nauto_mpg.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\ncar_name\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504.0\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165.0\n3693.0\n11.5\n70\n1\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150.0\n3436.0\n11.0\n70\n1\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150.0\n3433.0\n12.0\n70\n1\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140.0\n3449.0\n10.5\n70\n1\nford torino\n\n\n\n\n\n\n\nThe mpg column contains the value that we want to predict (it is our target column). We‚Äôll use the other numeric columns as random variables (predictors).\nIt will make things easier if we create variables to contain the name of the target column and the random variables. These can be used when we interact with Pandas DataFrames to quickly select those columns by name.\n\ntarget_col = \"mpg\"\nrandom_var_cols = [\n    \"cylinders\",\n    \"displacement\",\n    \"horsepower\",\n    \"weight\",\n    \"acceleration\",\n    \"model_year\"\n]\n\nLet‚Äôs use the info() DataFrame method to see what kinds of values we have, and whether there are any missing values.\n\nauto_mpg.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 398 entries, 0 to 397\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           398 non-null    float64\n 1   cylinders     398 non-null    int64  \n 2   displacement  398 non-null    float64\n 3   horsepower    398 non-null    object \n 4   weight        398 non-null    float64\n 5   acceleration  398 non-null    float64\n 6   model_year    398 non-null    int64  \n 7   origin        398 non-null    int64  \n 8   car_name      398 non-null    object \ndtypes: float64(4), int64(3), object(2)\nmemory usage: 28.1+ KB\n\n\nü§î We notice that the ‚Äòhorsepower‚Äô column is not recognized as numeric (the type is reported as ‚Äúobject‚Äù). Let‚Äôs check if there are any non-numeric values.\nOne way to do that is to look at the values. Another is to write a function to check if each value can be converted to a float, and return True if it can or False otherwise. Then, we can just filter for values that will not convert:\n\ndef is_number(x):\n    \"\"\"Returns True if `x` is a number, or False otherwise.\"\"\"\n    try:\n        float(x)\n    except ValueError:\n        return False\n    return True\n# List the values and counts for all non-numeric values in the 'horsepower' variable:\nauto_mpg.loc[auto_mpg['horsepower'].apply(is_number) == False, 'horsepower'].value_counts()\n\nhorsepower\n?    6\nName: count, dtype: int64\n\n\nFrom this, we can see that the only non-numeric value is ‚Äò?‚Äô. So, we could convert the column to numeric after replacing the ‚Äò?‚Äô values with na.nan, or we can just reload the DataFrame from the CSV file and tell Pandas to treat ‚Äò?‚Äô as NA. We will do the latter to demonstrate how to do it, and because this dataset is small enough that it will not take long to do it that way. (For larger data, the former approach would probably be faster.)\n\n# Re-load the dataset, treating '?' as NA values.\nauto_mpg = pd.read_csv('auto-mpg.data', sep=r'\\s+', names=column_names, na_values='?')\n# Check the info again\nauto_mpg.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 398 entries, 0 to 397\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           398 non-null    float64\n 1   cylinders     398 non-null    int64  \n 2   displacement  398 non-null    float64\n 3   horsepower    392 non-null    float64\n 4   weight        398 non-null    float64\n 5   acceleration  398 non-null    float64\n 6   model_year    398 non-null    int64  \n 7   origin        398 non-null    int64  \n 8   car_name      398 non-null    object \ndtypes: float64(5), int64(3), object(1)\nmemory usage: 28.1+ KB\n\n\nNow, we have correct data types. We will still have missing values in ‚Äòhorsepower‚Äô though, so we need to remove those rows:\n‚ÑπÔ∏è Pandas has a method called dropna() that can drop missing values.\nIn this case, we want to call it with the subset argument set to our random variable cols (random_var_cols) and the axis argument set to 0.\n\n# Now drop rows with missing values\nauto_mpg = auto_mpg.dropna(subset=random_var_cols, axis=0)\n\n\n\nüìä Visualize Early, Visualize Often\nLet‚Äôs take a look at the dataset. We will plot some relationships between our target variable (mpg) and the predictors. (This will take several seconds.)\n\nsns.pairplot(auto_mpg[random_var_cols + [target_col]], corner=True)\n\n\n\n\n\n\n\n\nLooking at the pairplot, we can see that there are clear relationships between MPG and the various predictors (bottom row). For example, as weight increases, MPG tends to decrease. Similarly, as horsepower and displacement increase, MPG tends to decrease. These relationships make intuitive sense: heavier cars with more powerful engines typically consume more fuel.\nWe can also see that some of the predictors have relationships with one another, which could be a problem for models that assume independent variables. Let‚Äôs get a sense of that by plotting the (Spearman) correlation between all the variables:\n\n# Calculate correlation matrix.  We use Spearman because we observed some non-linear relationships in the pairplot.\ncorrelation_matrix = auto_mpg[random_var_cols + [target_col]].corr('spearman')\n\n# Plot the correlation matrix as a heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\nThe correlation matrix confirms our observations from the scatter plots. We can see strong negative correlations between MPG and weight, displacement, and horsepower. This suggests that these variables will be important predictors in our regression models.\n‚ú® Choosing the right random variables for prediction is vital. This is why it is a good idea to get to know your dataset early in the process! Visualize early, visualize often!"
  },
  {
    "objectID": "notebooks/python_intermediate_regression_complete_ASRI25.html#lets-see-how-well-we-can-predict-mpg-with-a-linear-model.",
    "href": "notebooks/python_intermediate_regression_complete_ASRI25.html#lets-see-how-well-we-can-predict-mpg-with-a-linear-model.",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "Let‚Äôs see how well we can predict MPG with a linear model.",
    "text": "Let‚Äôs see how well we can predict MPG with a linear model.\nFirst, we‚Äôll use the LinearRegression model from scikit-learn.\nBased on the visualizations above, let‚Äôs start with a simple model using just weight and displacement as predictors:\n\nsimple_random_var_cols = [\n    \"weight\",\n    \"displacement\",\n]\n\nFor scoring, we will use R¬≤ and Mean Absolute Error (MAE).\n\nscoring_metrics = {\n    'r2': 'r2',\n    'mae': 'neg_mean_absolute_error',\n}\n\nTo quickly determine if a linear model will be suitable for this problem, we can use the cross_validate() function from Scikit-Learn. This function wraps up a lot of functionality. It will set up a k-fold cross validation experiment (with default of \\(k=5\\), for five-fold CV). It will take the model of your choice and automatically train the model for each training fold, then predict the test cases and score the predictions on the test folds (with the R¬≤ metric by default for regression).\nThe scores for each fold are returned. We can calculate and report the mean score over all five folds along with the standard deviation of the scores to see whether the model is able to do a good job in general, and how much variation we would expect for different training sets. Models should have high R¬≤ values, and a low standard deviation would indicate that the model generalizes to new data very well. (A high standard deviation would indicate the model is unstable and doesn‚Äôt generalize well.) For the Mean Absolute Error metric, we want smaller values (less deviation from the true mpg, and a small standard deviation.\nThe linear model will look like:\n\\[\ny_{mpg} = \\beta_0 + \\beta_1 x_{weight} + \\beta_2 x_{displacement}\n\\]\n\nscores = cross_validate(\n    LinearRegression(), X=auto_mpg[simple_random_var_cols], y=auto_mpg[target_col],\n    scoring=scoring_metrics\n)\nprint(\n    f\"mean R¬≤ : {scores['test_r2'].mean():0.3f}, std: {scores['test_r2'].std():0.3f}\\n\"\n    f\"mean MAE: {-scores['test_mae'].mean():0.1f} mpg, std: {scores['test_mae'].std():0.1f} mpg.\"\n)  # print mean and standard deviation of score metrics\n\nmean R¬≤ : 0.326, std: 0.523\nmean MAE: 3.8 mpg, std: 1.4 mpg.\n\n\nüòû The two-variable linear model seems pretty unstable at this task. Look closely at the R¬≤ mean: 0.326 is a pretty low R¬≤ value to begin with, and the standard deviation over the 5 runs of the cross-validation experiment was 0.523, which is larger than the mean value! That means that on average, the linear model can explain only about 33% of the variance in the data, and the high standard deviation means that the model is not generalizing very well.\nü§î We might have expected this if we look back at our pair plots of weight and displacement versus mpg. Do you see the ‚Äúcurve‚Äù to the scatter? That is a good indicator that a linear model might not be ideal for this task.\nLet‚Äôs scale the model up to use all of our predictors.\nFor now, we will stick with a linear model. But, let‚Äôs use all of our random variables to see if that improves the result.\n\nscores = cross_validate(\n    LinearRegression(), X=auto_mpg[random_var_cols], y=auto_mpg[target_col],\n    scoring=scoring_metrics\n)\nprint(\n    f\"mean R¬≤ : {scores['test_r2'].mean():0.3f}, std: {scores['test_r2'].std():0.3f}\\n\"\n    f\"mean MAE: {-scores['test_mae'].mean():0.1f} mpg, std: {scores['test_mae'].std():0.1f} mpg.\"\n)  # print mean and standard deviation of score metrics\n\nmean R¬≤ : 0.571, std: 0.231\nmean MAE: 3.1 mpg, std: 0.8 mpg.\n\n\nThis provided a modest improvement, both in R¬≤ and standard deviation. We see a smaller change in MAE and its standard deviation, but still an improvement nonetheless.\nLet‚Äôs try a non-linear model. Let‚Äôs consider a quadratic model (a polynomial model of degree 2). The model will look like:\n\\[\ny_{mpg} = \\beta_0 + \\beta_1 x_{weight} + \\beta_2 x_{displacement} + \\beta_3 x_{weight}^2 + \\beta_4 x_{displacement}^2\n\\]\nThe way we do this is to pre-compute the polynomial features \\(x_{weight}^2\\) and \\(x_{displacement}^2\\), then use a linear regression model as before.\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nscores = cross_validate(\n    LinearRegression(), X=poly.fit_transform(auto_mpg[random_var_cols]), y=auto_mpg[target_col],\n    scoring=scoring_metrics\n)\nprint(\n    f\"mean R¬≤ : {scores['test_r2'].mean():0.3f}, std: {scores['test_r2'].std():0.3f}\\n\"\n    f\"mean MAE: {-scores['test_mae'].mean():0.1f} mpg, std: {scores['test_mae'].std():0.1f} mpg.\"\n)  # print mean and standard deviation of score metrics\n\nmean R¬≤ : 0.679, std: 0.222\nmean MAE: 2.6 mpg, std: 1.0 mpg.\n\n\nüéâ Now we see a better result. R¬≤ above 65% is starting to look more promising (but not necessarily ‚Äúgood‚Äù). We see that on average our model is off by about 2.6 mpg. If that is an acceptable error amount, we might be happy with this one.\nNow, let‚Äôs take a look at a different kind of model, just for comparison. A Random Forest model is a non-linear model that works well for lots of tasks. Scikit-Learn provides one called RandomForestRegressor for regression problems.\nLet‚Äôs try it in exactly the same experimental setup we used for the linear model.\n\nscores = cross_validate(\n    RandomForestRegressor(random_state=1),\n    X=auto_mpg[random_var_cols],\n    y=auto_mpg[target_col],\n    scoring=scoring_metrics\n)\nprint(\n    f\"mean R¬≤ : {scores['test_r2'].mean():0.3f}, std: {scores['test_r2'].std():0.3f}\\n\"\n    f\"mean MAE: {-scores['test_mae'].mean():0.1f} mpg, std: {scores['test_mae'].std():0.1f} mpg.\"\n)  # print mean and standard deviation of score metrics\n\nmean R¬≤ : 0.748, std: 0.198\nmean MAE: 2.2 mpg, std: 0.8 mpg.\n\n\nThe random forest did even better than the quadratic model! This suggests that there might be non-linear relationships in the data that the random forest is able to capture better than either the linear or quadratic models.\nOne note:\nWe used random_state=1 to seed the random number generator within the model, causing it to produce identical results if we train it again on the same data. Random forests (as implied by their name) rely on some randomness during training, so you don‚Äôt expect to get the same performance every time. This makes reproducible results difficult.\nüí° By seeding the random state, we ‚Äúlock‚Äù it to a specific outcome (assuming no external changes). This way, others can reproduce our results in the future."
  },
  {
    "objectID": "notebooks/python_intermediate_regression_complete_ASRI25.html#exploring-more-ways-to-characterize-regressor-performance.",
    "href": "notebooks/python_intermediate_regression_complete_ASRI25.html#exploring-more-ways-to-characterize-regressor-performance.",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "Exploring more ways to characterize regressor performance.",
    "text": "Exploring more ways to characterize regressor performance.\n\nüìä Visualize!\nWhen evaluating regression models, it‚Äôs important to look at the residuals (the differences between predicted and actual values). A good regression model should have residuals that are randomly distributed around zero.\nLet‚Äôs split our data into training and testing sets, train our models, and then visualize the residuals:\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    auto_mpg[random_var_cols], auto_mpg[target_col], test_size=0.2, random_state=42\n)\n\n# Train the linear regression model\nlinear_model = LinearRegression().fit(X_train, y_train)\n\n# Make predictions on the test set\nlinear_preds = linear_model.predict(X_test)\n\n# Calculate residuals\nlinear_residuals = y_test - linear_preds\n\n# Plot the residuals\nplt.figure(figsize=(10, 6))\nplt.scatter(linear_preds, linear_residuals)\nplt.axhline(y=0, color='r', linestyle='-')\nplt.xlabel('Predicted MPG')\nplt.ylabel('Residuals')\nplt.title('Residual Plot for Linear Regression')\nplt.show()\n\n\n\n\n\n\n\n\n‚ú® Interpreting a residual plot: To interpret a residual plot like the one above, we look at two things: (1) The magnitude of the residuals, which represents how far our predictions are from the actual values. We want the dots to be close to the zero line. (2) The shape of the residuals. The plotted residuals are ordered by magnitude of the prediction from smallest to largest, covering the range of predicted values. What we want is to see no trend or ‚Äúpattern‚Äù to the scatter of residuals versus the zero line. If we see a trend or pattern, then it is a clue that our model is not making the same mistakes across the range of its outputs, and so it might not be a good fit for the application.\nHere, we see that there is a ‚Äúcurve‚Äù in the residuals‚Äîthey start above the zero line, trend downward, then back up again (a ‚Äúsmile‚Äù pattern). This is a clue that the actual target variable ‚Äúmpg‚Äù is probably not a linear function of the predictors. A non-linear model might work better.\nNow let‚Äôs do the same plot for the Random Forest model:\n\n# Train the random forest model\nrf_model = RandomForestRegressor(random_state=1).fit(X_train, y_train)\n\n# Make predictions on the test set\nrf_preds = rf_model.predict(X_test)\n\n# Calculate residuals\nrf_residuals = y_test - rf_preds\n\n# Plot the residuals\nplt.figure(figsize=(10, 6))\nplt.scatter(rf_preds, rf_residuals)\nplt.axhline(y=0, color='r', linestyle='-')\nplt.xlabel('Predicted MPG')\nplt.ylabel('Residuals')\nplt.title('Residual Plot for Random Forest')\nplt.show()\n\n\n\n\n\n\n\n\nHere, we don‚Äôt see as much of a pattern. That is a good thing. For the most part, our Random Forest model seems to be making similar errors across its range, with two possible exceptions: The smaller predictions seem to be better than the larger ones in general, and there is a rough patch between about 22 and 32 mpg. where the model seems to be over-predicting more than everywhere else. An even more powerful model might be able to do a better job, but we won‚Äôt investigate that in this workshop.\nüí° Another way to visualize the errors: Let‚Äôs also compare the actual vs.¬†predicted values for both models:\n\n# Create a figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot actual vs predicted for Linear Regression\nax1.scatter(y_test, linear_preds)\nax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nax1.set_xlabel('Actual MPG')\nax1.set_ylabel('Predicted MPG')\nax1.set_title('Linear Regression: Actual vs Predicted')\n\n# Plot actual vs predicted for Random Forest\nax2.scatter(y_test, rf_preds)\nax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nax2.set_xlabel('Actual MPG')\nax2.set_ylabel('Predicted MPG')\nax2.set_title('Random Forest: Actual vs Predicted')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: To interpret these plots, keep in mind that the dotted red line represents a ‚Äúperfect fit‚Äù model. We don‚Äôt expect every dot to be on the line, but we want them close and randomly spread around it (no patterns). On the left plot (the linear model), we see a ‚Äúbent‚Äù scatter of points compared to the line, indicating a bad fit. On the right, the Random Forest (RF) behaves better, but we can see the over-prediction in the mid-range and then the beginnings of under-prediction happing at the top end (similar to what we see on the linear model‚Äôs graph). Again, the RF looks better, but leaves room for improvement.\n\n\nNon-visual metrics\nLet‚Äôs look at other regression metrics.\nScikit-Learn provides several metrics appropriate for evaluating regression models. You can see the list at https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics.\nWe‚Äôll calculate some common metrics for both models:\n\n# Calculate metrics for Linear Regression\nlinear_mse = mean_squared_error(y_test, linear_preds)\nlinear_rmse = np.sqrt(linear_mse)\nlinear_mae = mean_absolute_error(y_test, linear_preds)\nlinear_msd = np.mean(linear_preds-y_test)\nlinear_r2 = r2_score(y_test, linear_preds)\n\n# Calculate metrics for Random Forest\nrf_mse = mean_squared_error(y_test, rf_preds)\nrf_rmse = np.sqrt(rf_mse)\nrf_mae = mean_absolute_error(y_test, rf_preds)\nrf_msd = np.mean(rf_preds-y_test)\nrf_r2 = r2_score(y_test, rf_preds)\n\n# Create a DataFrame to display the metrics\nmetrics_df = pd.DataFrame({\n    'Model': ['Linear Regression', 'Random Forest'],\n    'MSE': [linear_mse, rf_mse],\n    'RMSE': [linear_rmse, rf_rmse],\n    'MAE': [linear_mae, rf_mae],\n    'MSD': [linear_msd, rf_msd],\n    'R¬≤': [linear_r2, rf_r2]\n})\n\nmetrics_df\n\n\n\n\n\n\n\n\nModel\nMSE\nRMSE\nMAE\nMSD\nR¬≤\n\n\n\n\n0\nLinear Regression\n10.502370\n3.240736\n2.503860\n0.179652\n0.794235\n\n\n1\nRandom Forest\n5.876812\n2.424214\n1.732253\n0.409367\n0.884860\n\n\n\n\n\n\n\nLet‚Äôs understand these metrics:\n\nMean Squared Error (MSE): The average of the squared differences between predicted and actual values. Lower is better.\nRoot Mean Squared Error (RMSE): The square root of MSE. It‚Äôs in the same units as the target variable, making it more interpretable. Lower is better.\nMean Absolute Error (MAE): The average of the absolute differences between predicted and actual values. Lower is better.\nMean Signed Deviation (MSD): The average of the differences between predicted and actual values, retaining the sign. Closer to zero is better, and the sign indicates the direction of the bias (e.g.¬†model is over-predicting vs.¬†under-predicting).\nR¬≤ (Coefficient of Determination): Represents the proportion of variance in the dependent variable that is predictable from the independent variables. Ranges from 0 to 1, with higher values indicating better fit.\n\nBased on these metrics, the Random Forest model outperforms the Linear Regression model on our test set. However, MSD reveals that the RF model is a bit more biased toward over-predicting the mpg than the linear model. So, when RF makes a mistake, it is more likely to favor higher mpg.\n\n\nFeature Importance\nSome models can provide information about the importance of each featuer, allowing us to understand the underlying process better and perhaps perform feature selection to simplify our models. Different models do this differently.\nLinear Regression models convey feature importance in the magnitude of the coefficients they compute for each feature. You can access this information for Scikit-Learn LinearRegression models in the coef_ attribute:\n\n# We will get the absolute values of the coefficients, since we don't care about direction here\nlinear_feature_importance = np.abs(linear_model.coef_)\n# And then normalize them so that they sum to 1.0 just to make the scale easier to interpret.\nlinear_feature_importance /= np.sum(linear_feature_importance)\n\n# Create a DataFrame to display feature importance\nlinear_importance_df = pd.DataFrame({\n    'Feature': random_var_cols,\n    'Importance': linear_feature_importance\n})\n\n# Sort by importance\nlinear_importance_df = linear_importance_df.sort_values('Importance', ascending=False)\n\n# Plot feature importance\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=linear_importance_df)\nplt.title('Feature Importance from Linear Regression Model')\nplt.xlabel(\"Relative Importance\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nüíπ We can see from this that ‚Äúmodel_year‚Äù had the most impact on the linear model. Looking back at the original scatterplots, we can see that model year does have a mostly-linear trend where mpg tends to increase in more recent years.\nThe RandomForestRegressor model provides feature importance information in its feature_importances_ attribute:\n\n# Get feature importances from the Random Forest model (it is already normalized to sum to 1.0).\nrf_feature_importance = rf_model.feature_importances_\n\n# Create a DataFrame to display feature importance\nimportance_df = pd.DataFrame({\n    'Feature': random_var_cols,\n    'Importance': rf_feature_importance\n})\n\n# Sort by importance\nimportance_df = importance_df.sort_values('Importance', ascending=False)\n\n# Plot feature importances\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=importance_df)\nplt.title('Feature Importances from Random Forest Model')\nplt.xlabel(\"Relative Importance\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nü§î Notice that the RF model has a different conclusion about which feature is most important, choosing ‚Äúdisplacement‚Äù. Displacement seemed highly correlated with mpg in the original scatterplot, but the relationship is highly non-linear, and has redundancy with ‚Äúcylinders‚Äù and ‚Äúhorsepower‚Äù. The RF was able to use this variable in spite of these challenges."
  },
  {
    "objectID": "notebooks/python_intermediate_regression_complete_ASRI25.html#working-with-categorical-features",
    "href": "notebooks/python_intermediate_regression_complete_ASRI25.html#working-with-categorical-features",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "Working with categorical features",
    "text": "Working with categorical features\nSo far, we‚Äôve only used numeric features for our predictors. But the Auto MPG dataset also contains a categorical feature:\n\norigin - three levels: [1, 2, 3] representing American, European, and Japanese cars respectively\n\nLet‚Äôs convert this numeric encoding to more meaningful labels first:\n\n# Create a mapping dictionary\norigin_map = {1: 'American', 2: 'European', 3: 'Japanese'}\n\n# Create a new column with the mapped values\nauto_mpg['origin_name'] = auto_mpg['origin'].map(origin_map)\n\n# Display the first few rows to verify\nauto_mpg[['origin', 'origin_name']].head()\n\n\n\n\n\n\n\n\norigin\norigin_name\n\n\n\n\n0\n1\nAmerican\n\n\n1\n1\nAmerican\n\n\n2\n1\nAmerican\n\n\n3\n1\nAmerican\n\n\n4\n1\nAmerican\n\n\n\n\n\n\n\nNow let‚Äôs visualize how MPG varies by origin:\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='origin_name', y='mpg', data=auto_mpg)\nplt.title('MPG by Car Origin')\nplt.xlabel('Origin')\nplt.ylabel('MPG')\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that there are differences in MPG based on the car‚Äôs origin. Japanese cars tend to have higher MPG, followed by European cars, with American cars having the lowest MPG on average.\nTo include this categorical variable in our regression models, we need to encode it. One common approach is one-hot encoding.\nOne-hot encoding is an encoding technique in which a variable with \\(N\\) levels is split into \\(N\\) new pseudo-variables where each is a binary variable encoded as 1 or 0.\nLet‚Äôs see how our origin variable might look if it were one-hot encoded:\nBefore\nmpg  cylinders  displacement  ...  origin  car_name\n20.5  6         200.0         ...  1       chevrolet malibu\n15.0  8         350.0         ...  1       buick skylark 320\n22.0  4         121.0         ...  2       volkswagen 411 (sw)\n26.0  4         98.00         ...  2       fiat 124 sport coupe\n32.0  4         71.00         ...  3       toyota corolla 1200\n24.0  4         120.0         ...  3       honda civic\nAfter\nmpg  cylinders  displacement  ...  origin_American  origin_European  origin_Japanese  car_name\n20.5  6         200.0         ...  1                0                0                chevrolet malibu\n15.0  8         350.0         ...  1                0                0                buick skylark 320\n22.0  4         121.0         ...  0                1                0                volkswagen 411 (sw)\n26.0  4         98.00         ...  0                1                0                fiat 124 sport coupe\n32.0  4         71.00         ...  0                0                1                toyota corolla 1200\n24.0  4         120.0         ...  0                0                1                honda civic\nHere‚Äôs the code:\nPandas can do this in a dataframe by using the get_dummies() method. You provide a prefix (like \"origin\") and the existing levels are used to complete the new column names.\n\nsample_rows = [253, 1, 77, 114, 131, 149] # this lets us select the same cars shown above\nauto_mpg_encoded = pd.get_dummies(        # get_dummies converts to one-hot encoding\n    auto_mpg, columns=[\"origin_name\"], prefix=\"origin\", dtype=int\n)\nauto_mpg_encoded.loc[sample_rows][['mpg', 'origin_American', 'origin_European', 'origin_Japanese', 'car_name']]\n\n\n\n\n\n\n\n\nmpg\norigin_American\norigin_European\norigin_Japanese\ncar_name\n\n\n\n\n253\n20.5\n1\n0\n0\nchevrolet malibu\n\n\n1\n15.0\n1\n0\n0\nbuick skylark 320\n\n\n77\n22.0\n0\n1\n0\nvolkswagen 411 (sw)\n\n\n114\n26.0\n0\n1\n0\nfiat 124 sport coupe\n\n\n131\n32.0\n0\n0\n1\ntoyota corolla 1200\n\n\n149\n24.0\n0\n0\n1\nhonda civic\n\n\n\n\n\n\n\nNow let‚Äôs use these one-hot encoded features in our regression models:\n\n# Define columns including one-hot encoded origin\nall_features = random_var_cols + ['origin_American', 'origin_European', 'origin_Japanese']\n\nNow we will evaluate each of our models with a 5-fold CV like we did in the beginning, and compare the original features to the ones with origin information included:\n\n# create a helper function so we don't have to repeat this code too much\ndef do_evaluation(model, X, y, caption):\n    scores = cross_validate( model, X, y, scoring=scoring_metrics)\n    print(\n        f\"{caption}\\n\"\n        f\"mean R¬≤ : {scores['test_r2'].mean():0.3f}, std: {scores['test_r2'].std():0.3f}\\n\"\n        f\"mean MAE: {-scores['test_mae'].mean():0.1f} mpg, std: {scores['test_mae'].std():0.2f} mpg.\\n\"\n    )  # print mean and standard deviation of score metrics\n\n\ndo_evaluation(LinearRegression(), X=auto_mpg[random_var_cols], y=auto_mpg[target_col], caption=\"Linear Regression without origin info:\")\ndo_evaluation(LinearRegression(), X=auto_mpg_encoded[all_features], y=auto_mpg_encoded[target_col], caption=\"Linear Regression including origin info:\")\n\nprint(\"---\\n\")\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\ndo_evaluation(LinearRegression(), X=poly.fit_transform(auto_mpg[random_var_cols]), y=auto_mpg[target_col], caption=\"Polynomial (d=2) Regression without origin info:\")\ndo_evaluation(LinearRegression(), X=poly.fit_transform(auto_mpg_encoded[all_features]), y=auto_mpg_encoded[target_col], caption=\"Polynomial (d=2) Regression including origin info:\")\n\nprint(\"---\\n\")\n\ndo_evaluation(RandomForestRegressor(random_state=1), X=auto_mpg[random_var_cols], y=auto_mpg[target_col], caption=\"Random Forest regression without origin info:\")\ndo_evaluation(RandomForestRegressor(random_state=1), X=auto_mpg_encoded[all_features], y=auto_mpg_encoded[target_col], caption=\"Random Forest regression including origin info:\")\n\nLinear Regression without origin info:\nmean R¬≤ : 0.571, std: 0.231\nmean MAE: 3.1 mpg, std: 0.76 mpg.\n\nLinear Regression including origin info:\nmean R¬≤ : 0.594, std: 0.197\nmean MAE: 3.0 mpg, std: 0.68 mpg.\n\n---\n\nPolynomial (d=2) Regression without origin info:\nmean R¬≤ : 0.679, std: 0.222\nmean MAE: 2.6 mpg, std: 0.97 mpg.\n\nPolynomial (d=2) Regression including origin info:\nmean R¬≤ : 0.661, std: 0.210\nmean MAE: 2.6 mpg, std: 0.85 mpg.\n\n---\n\nRandom Forest regression without origin info:\nmean R¬≤ : 0.748, std: 0.198\nmean MAE: 2.2 mpg, std: 0.76 mpg.\n\nRandom Forest regression including origin info:\nmean R¬≤ : 0.752, std: 0.194\nmean MAE: 2.2 mpg, std: 0.77 mpg.\n\n\n\nIncluding the origin as a categorical feature has improved our model performance in the linear regression and quadratic models, but not significantly. The RF model was mostly unchanged by the addition of this variable.\nHowever, categorical variables can be very important in some datasets! Consider encoding them and using them in the model; you can evaluate whether or not it was worthwhile before training your final model version.\n\nThere‚Äôs More Than One Way to Do It\nYou can also use the OneHotEncoder from Scikit-Learn to encode categorical variables. It‚Äôs particularly useful when you don‚Äôt want to modify your original dataframe and prefer to create a ‚Äúdata pipeline‚Äù for preprocessing your data during training or inference.\nIn fact, there are several other approaches to encoding categorical values.\nYou can learn a lot more here: https://www.kaggle.com/code/arashnic/an-overview-of-categorical-encoding-methods"
  },
  {
    "objectID": "notebooks/python_intermediate_regression_complete_ASRI25.html#thank-you",
    "href": "notebooks/python_intermediate_regression_complete_ASRI25.html#thank-you",
    "title": "Regression Techniques in Python (Intermediate)",
    "section": "Thank You!",
    "text": "Thank You!\nThis notebook in tutorial and completed form is available at:\nhttps://jcausey-astate.github.io/ASRI-2025/"
  },
  {
    "objectID": "python_intermediate_clustering_complete_ASRI25.html#asri-2025",
    "href": "python_intermediate_clustering_complete_ASRI25.html#asri-2025",
    "title": "Clustering in Python (Intermediate)",
    "section": "ASRI 2025",
    "text": "ASRI 2025\n\n\n\nClassification in Python (Intermediate)\n\n\nThis notebook shows some introductory examples from the ‚ÄúClustering in Python‚Äù workshop session.\nThe notebook uses the following modules:\n\nmatplotlib : Provides basic graphing/charting.\nnumpy : Allows matrix and vector/array math.\npandas : Provides DataFrame functionality.\nscipy : SciPy provides algorithms for optimization, algebraic equations, statistics and many other classes of problems. We will use it for building dendrograms.\nseaborn : Works with matplotlib to provide nicer graphs.\nsklearn : Scikit-Learn provides machine learning and data manipulation tools.\n\nWe will rely heavily on the Scikit-Learn library for models, metrics, and experimental design tools. See the full documentation for this fantastic library at https://scikit-learn.org.\n\nClustering is an unsupervised learning technique for exploring relationships between the random variables in a dataset. We use a clustering analysis to try to identify groups or similar objects in datasets with two or more random variables.\nIn this tutorial, we will look at some commonly-used clustering techniques:\n\nk-Means Clustering\n\nProbably the most common clustering technique.\nGood for finding clusters that look like ‚Äúblobs‚Äù when visualized.\nYou need to know the number of clusters in advance.\n\nMean Shift Clustering\n\nDoesn‚Äôt require assumptions about the number of clusters.\nSomewhat robust when clusters are not simple ‚Äúblobs‚Äù.\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nClusters areas of similar density, regardless of cluster ‚Äúshape‚Äù.\nWorks for situations where clusters don‚Äôt look like separate ‚Äúblobs‚Äù.\nYou don‚Äôt need to know the number of clusters in advance.\n\nAgglomerative Hierarchical Clustering\n\nEstablishes relationships at all levels of the comparison metric between all samples.\nCan be visualized as a dendrogram.\nAllows you to determine the ‚Äúright‚Äù number of clusters by examining the dendrogram.\n\nGaussian Mixture Model\n\nAssumes the data can be represented as some number of multi-dimensional Gaussian distributions.\nWorks well when the data are ‚Äúblob‚Äù shaped, even when they overlap ‚Äì especially if the density of the blobs differ.\nIdeally, you need to know how many ‚Äúblobs‚Äù to expect.\n\n\n‚ÑπÔ∏è More information about these techniques and many others is available at https://scikit-learn.org/stable/modules/clustering.html."
  },
  {
    "objectID": "python_intermediate_clustering_complete_ASRI25.html#lets-see-some-code",
    "href": "python_intermediate_clustering_complete_ASRI25.html#lets-see-some-code",
    "title": "Clustering in Python (Intermediate)",
    "section": "Let‚Äôs see some code",
    "text": "Let‚Äôs see some code\nFirst, we have to import the modules, objects, and functions we will be using in this tutorial:\n\n# Ensure that Scikit Learn is recent enough to include HDBSCAN:\n!pip install \"scikit-learn&gt;=1.3\"\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn import cluster\nfrom sklearn import datasets\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import HDBSCAN\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MeanShift\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "python_intermediate_clustering_complete_ASRI25.html#comparing-clustering-techniques",
    "href": "python_intermediate_clustering_complete_ASRI25.html#comparing-clustering-techniques",
    "title": "Clustering in Python (Intermediate)",
    "section": "Comparing clustering techniques",
    "text": "Comparing clustering techniques\nüí° There is no single perfect clustering algorithm. Different algorithms make different assumptions with regards to what you already know about your dataset.\nFor that reason, we will use a few different datasets to illustrate the strengths and weaknesses of each clustering algorithm.\nHere are the datasets we will use:\n\nPalmer Penguins : This real-world dataset contains measurements of the bill length versus the flipper length of three species of penguins at the Palmer Antarctic research station. https://allisonhorst.github.io/palmerpenguins/articles/intro.html\nMoons : This simulated dataset consists of two crescent ‚Äúmoon‚Äù shapes, with one inverted so that there is no way to linearly separate the two groups, but they are visually separate.\nCircles : This simulated dataset consists of two concentric rings, representing noisy values that are in one of two groups, assigned by the radius of the ring in which the sample is located.\nDensity Blobs : This simulated dataset will consist of three ‚Äúblobs‚Äù with widely varying density and size characteristics. The blobs overlap so that they are not perfectly visually separable.\n\nThe following code block creates the simulated datasets and loads the Palmer Penguins dataset and selects the two predictors we will use for this tutorial.\n\nn_samples = 500\n\n# penguins : this is a real dataset, so we will load it and select a useful set of\n#            features.  We will also store the labels.\npenguins = sns.load_dataset(\"penguins\").dropna(\n    axis=0, subset=[\"bill_length_mm\", \"flipper_length_mm\"]\n)\npenguins_encoder = LabelEncoder().fit(penguins[\"species\"].values)\npenguins_classes = penguins_encoder.classes_\npenguins_labels = penguins_encoder.transform(penguins[\"species\"].values)\npenguins = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].values\npenguins = StandardScaler().fit_transform(penguins)\n\n# moons : two crescent \"moon\" shapes, with one inverted so that there is no way to\n# linearly separate the two groups, but they are visually separate.\nmoons, moons_labels = datasets.make_moons(n_samples=n_samples, noise=0.065)\nmoons = StandardScaler().fit_transform(moons)\n\n# circles : two concentric rings, representing noisy values that are in one of two\n# groups, assigned by the radius of the ring in which the sample is located.\ncircles, circles_labels = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.065\n)\ncircles = StandardScaler().fit_transform(circles)\n\n# density blobs : three \"blobs\" with widely varying density and size characteristics.\n# The blobs overlap so that they are not perfectly visually separable.\ndensity_blobs, density_blobs_labels = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.9, 0.5, 2.7], random_state=4\n)\ndensity_blobs = StandardScaler().fit_transform(density_blobs)\n\nüí° To make understanding the data easier, we need a way to visualize the datasets, plus give hints about what the ‚Äúexpected‚Äù labeling would be as well as what a clustering technique assigns as labels.\nWe will create a function that can take a list of datasets with corresponding names, labels, and styles. The function will use the Seaborn scatterplot() function to plot the datasets in a grid.\n\ndef plot_all(data_list, names=None, label_list=None, style_list=None):\n    \"\"\"\n    Plot scatterplots for all the 2-D datasets in `data_list`.\n    `names` provide a title for each plot\n    `label_list` is a list of label assignments for the samples in the data list.\n    `style_list` is a list of marker shape assignments for the samples in the data list.\n    \"\"\"\n    nrows = int(round(len(data_list) / 2, 0))\n    fig, axs = plt.subplots(nrows=nrows, ncols=2, figsize=(12, 10))\n    for r in range(nrows):\n        for c in range(2):\n            hue = label_list[r * 2 + c] if label_list is not None else None\n            style = style_list[r * 2 + c] if style_list is not None else None\n            title = names[r * 2 + c] if names is not None else None\n            sns.scatterplot(\n                x=data_list[r * 2 + c][:, 0],\n                y=data_list[r * 2 + c][:, 1],\n                hue=hue,\n                style=style,\n                palette=sns.color_palette(n_colors=len(set(hue))),\n                legend=False,\n                ax=axs[r, c],\n            ).set(title=title)\n    plt.subplots_adjust(wspace=0.50)\n    plt.show()\n\nNow, let‚Äôs use the plot_all() function to plot our four datasets. We will use the ‚Äúreal‚Äù labels to color the points so that you can see what the ‚Äúideal‚Äù labeling would be.\n\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\"Penguins\", \"Moons\", \"Circles\", \"Density Blobs\"],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nüí° Keep in mind that in many real-world clustering applications, we will not know what the ‚Äúcorrect‚Äù or ‚Äúideal‚Äù labels are.\nüí° That is the ‚Äúmain idea‚Äù of clustering: It is a technique for suggesting a possible labeling for a dataset by examining relationships between the samples.\n\nk-Means\nLet‚Äôs start with k-Means clustering. k-Means is probably the most well-known clustering technique because it is simple, intuitive to understand, and it works well for clustering lots of datasets where we can visually identify ‚Äúgroups‚Äù in a scatterplot.\nk-Means works by first choosing a value \\(k\\) that represents the desired number of clusters. You need to already know something about the dataset to correctly choose \\(k\\) (for example, you may visualize it first and choose the number that seem right from looking at the scatterplot). There are also some techniques for choosing \\(k\\) if you have no idea of what to try. (See: https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb)\nLet‚Äôs see how k-Means would label each of our test datasets. We ‚Äúknow‚Äù from looking at the scatterplots that there are three groups in the ‚Äúpenguins‚Äù and ‚Äúdensity blobs‚Äù datasets, and there are two groups in the ‚Äúmoons‚Äù and ‚Äúcircles‚Äù datasets. So, we will choose \\(k\\) for each dataset to match our expectation.\n\nkmeans_2 = KMeans(n_clusters=2, random_state=1)  # k-Means with 2 clusters\nkmeans_3 = KMeans(n_clusters=3, random_state=1)  # k-Means with 3 clusters\n\n# We apply k-Means to each dataset with the `fit_transform()` method.  We \"believe\"\n# that \"moons\" and \"circles\" should have k=2 and \"penguins\" and \"density_blobs\"\n# should have k=3.\n#\npenguins_kmeans = kmeans_3.fit_predict(penguins)\nmoons_kmeans = kmeans_2.fit_predict(moons)\ncircles_kmeans = kmeans_2.fit_predict(circles)\ndensity_blobs_kmeans = kmeans_3.fit_predict(density_blobs)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"k-Means Penguins (k=3)\",\n        \"k-Means Moons (k=2)\",\n        \"k-Means Circles (k=2)\",\n        \"k-Means Density Blobs (k=3)\",\n    ],\n    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î We can see that k-Means does a reasonable job of finding the three groups in the ‚Äúpenguins‚Äù dataset. One of those groups (the Chinstrap penguins) is difficult to separate even with more complicated methods. The method does a good job of separating the two ‚Äúeasier‚Äù groups (Adelie and Gentoo).\n‚ùì What if we chose \\(k\\) incorrectly? Let‚Äôs see by first assuming \\(k=2\\) for all four datasets:\n\n# What if we believe there are 2 clusters in each of the datasets?\n#\nkmeans_2 = KMeans(n_clusters=2, random_state=1)  # k-Means with 2 clusters\n\n# We apply k-Means to each dataset with the `fit_transform()` method.\n#\npenguins_kmeans = kmeans_2.fit_predict(penguins)\nmoons_kmeans = kmeans_2.fit_predict(moons)\ncircles_kmeans = kmeans_2.fit_predict(circles)\ndensity_blobs_kmeans = kmeans_2.fit_predict(density_blobs)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"k-Means Penguins (k=2)\",\n        \"k-Means Moons (k=2)\",\n        \"k-Means Circles (k=2)\",\n        \"k-Means Density Blobs (k=2)\",\n    ],\n    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\n‚ùì What do you think about these clusters?\nNow, let‚Äôs incorrectly assume \\(k=4\\) for all four datasets:\n\n# What if we believe there are 4 clusters in all of the datasets?\n#\nkmeans_4 = KMeans(n_clusters=4, random_state=1)  # k-Means with 4 clusters\n\n# We apply k-Means to each dataset with the `fit_transform()` method.\n#\npenguins_kmeans = kmeans_4.fit_predict(penguins)\nmoons_kmeans = kmeans_4.fit_predict(moons)\ncircles_kmeans = kmeans_4.fit_predict(circles)\ndensity_blobs_kmeans = kmeans_4.fit_predict(density_blobs)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Moons (k=4)\",\n        \"k-Means Circles (k=4)\",\n        \"k-Means Density Blobs (k=4)\",\n    ],\n    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\n‚ÑπÔ∏è With k-Means, the clustering is stochastic, meaning that it will be different each time you run it (unless you set the random_state to make it reproducible).\nLet‚Äôs see that in action:\n\n# What if we believe there are 4 clusters in all of the datasets?\n#\nkmeans_4 = KMeans(n_clusters=4)  # k-Means with 4 clusters, no fixed random state.\n\n# We apply k-Means to each dataset with the `fit_transform()` method.\n#\npenguins_kmeans_1 = kmeans_4.fit_predict(penguins)\npenguins_kmeans_2 = kmeans_4.fit_predict(penguins)\npenguins_kmeans_3 = kmeans_4.fit_predict(penguins)\npenguins_kmeans_4 = kmeans_4.fit_predict(penguins)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, penguins, penguins, penguins],\n    [\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Penguins (k=4)\",\n    ],\n    [penguins_kmeans_1, penguins_kmeans_2, penguins_kmeans_3, penguins_kmeans_4],\n    [penguins_labels, penguins_labels, penguins_labels, penguins_labels],\n)\n\n\n\n\n\n\n\n\nüí° Keep in mind when using k-Means: A single run does not give you any indication confidence that the labeling was stable. (A stable labeling is an indication that the labeling is more likely ‚Äúcorrect‚Äù with respect to some real-world relationship.)\nYou can run the clustering several times and compare the stability of the cluster assignments to get an idea of whether the algorithms is seeing ‚Äúreal‚Äù groups or just randomly assigning them based on the initial seed points chosen.\nFor more information on measuring the stability of a clustering assignment, see the following article:\nhttps://amueller.github.io/aml/04-model-evaluation/17-cluster-evaluation.html\n\n\nMean-Shift Clustering\nMean-Shift clustering provides a powerful technique for clustering a dataset when we don‚Äôt already know the number of clusters.\n\n# For each dataset, we need to estimate the bandwidth parameter, then create the\n# MeanShift object.  To automate this, we will wrap the two steps up into a function.\ndef build_MeanShift(X, quantile=0.25):\n    bandwidth = cluster.estimate_bandwidth(X, quantile=quantile)\n    return MeanShift(bandwidth=bandwidth)\n\n\n# Now build a MeanShift clusterer for each dataset and use it to fit/predict.\npenguins_ms = build_MeanShift(penguins).fit_predict(penguins)\nmoons_ms = build_MeanShift(moons).fit_predict(moons)\ncircles_ms = build_MeanShift(circles, quantile=0.2).fit_predict(circles)\ndensity_blobs_ms = build_MeanShift(density_blobs, quantile=0.1).fit_predict(\n    density_blobs\n)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"Mean Shift Penguins\",\n        \"Mean Shift Moons\",\n        \"Mean Shift Circles\",\n        \"Mean Shift Density Blobs\",\n    ],\n    [penguins_ms, moons_ms, circles_ms, density_blobs_ms],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î Mean-Shift did a very good job on the Penguins dataset, without needing to know that there were three groups! It didn‚Äôt do so well with the other datasets though. Circles and Moons are not ‚Äúblob-like‚Äù enough for this technique to work well. In the density blobs dataset, it did (sort of) find the small dense blob‚Ä¶ But at the expense of finding many more small clusters that don‚Äôt really exist.\n‚ÑπÔ∏è The bandwidth parameter is the most important tuning parameter for this algorithm, and it can be tricky to get just right. The estimate_bandwidth() function is provided to help find a good bandwidth value, but it also has a tuning parameter quantile that can be a bit finicky. Some trial-and-error may be required to get results that seem correct.\n\n\nDBSCAN\nThe DBSCAN technique looks for regions of similar density, and assumes that those regions represent groups of samples. It is also capable of automatically identifying outliers (or ‚Äúnoise points‚Äù) that may not belong to any group.\n\n# Like MeanShift, there is an important tuning parameter ('eps') for DBSCAN.\n# Use DBSCAN to cluster each dataset:\npenguins_DBSCAN = DBSCAN(eps=0.25).fit_predict(penguins)\nmoons_DBSCAN = DBSCAN(eps=0.3).fit_predict(moons)\ncircles_DBSCAN = DBSCAN(eps=0.28).fit_predict(circles)\ndensity_blobs_DBSCAN = DBSCAN(eps=0.1).fit_predict(density_blobs)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"DBSCAN Penguins\",\n        \"DBSCAN Moons\",\n        \"DBSCAN Circles\",\n        \"DBSCAN Density Blobs\",\n    ],\n    [penguins_DBSCAN, moons_DBSCAN, circles_DBSCAN, density_blobs_DBSCAN],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î We can see that DBSCAN did a very good job on the ‚Äústrange‚Äù datasets (moons and circles). It also did a decent job with the Penguins dataset. The performance on the density blobs data is interesting: It identified the small dense cluster, but found spurious extra clusters as well. It also failed to see the two larger clusters as separate objects.\n‚ÑπÔ∏è DBSCAN requires careful tuning of the eps parameter, which is related to the expected density of clusters. When there is a wide variation in the density of ‚Äúreal‚Äù clusters, DBSCAN can fail to perform well (as it did here). However, if there is a sparse background of noisy points with some clusters of similar density, DBSCAN will find those clusters even if they are not ‚Äúblob-shaped‚Äù.\n\n\nHDBSCAN\nThe HDBSCAN technique performs DBSCAN over varying epsilon (eps) values and integrates the result to find a clustering that gives the best stability over epsilon. This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN), and be more robust to parameter selection.\n\n# HDBSCAN isn't as sensitive to tuning parameters as DBSCAN, although tuning the \n# `min_cluster_size` can be helpful if you have an idea of how many values should\n# be contained in even the smallest cluster.\n# Use HDBSCAN to cluster each dataset:\npenguins_HDBSCAN = HDBSCAN().fit_predict(penguins)\nmoons_HDBSCAN = HDBSCAN().fit_predict(moons)\ncircles_HDBSCAN = HDBSCAN().fit_predict(circles)\ndensity_blobs_HDBSCAN = HDBSCAN().fit_predict(density_blobs)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"HDBSCAN Penguins\",\n        \"HDBSCAN Moons\",\n        \"HDBSCAN Circles\",\n        \"HDBSCAN Density Blobs\",\n    ],\n    [penguins_HDBSCAN, moons_HDBSCAN, circles_HDBSCAN, density_blobs_HDBSCAN],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î HDBSCAN with defaults for all parameters did a pretty good job. It was really good at the circles and moons, but introduced some spurious extra clusters in penguins and density blobs.\n\n\nAgglomerative Hierarchical Clustering\n\n# Use agglomerative clustering to cluster each dataset:\npenguins_ag = AgglomerativeClustering(n_clusters=3, linkage=\"ward\").fit_predict(\n    penguins\n)\nmoons_ag = AgglomerativeClustering(n_clusters=2, linkage=\"single\").fit_predict(moons)\ncircles_ag = AgglomerativeClustering(n_clusters=2, linkage=\"single\").fit_predict(\n    circles\n)\ndensity_blobs_ag = AgglomerativeClustering(n_clusters=3, linkage=\"ward\").fit_predict(\n    density_blobs\n)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"Hierarchical Penguins\",\n        \"Hierarchical Moons\",\n        \"Hierarchical Circles\",\n        \"Hierarchical Density Blobs\",\n    ],\n    [penguins_ag, moons_ag, circles_ag, density_blobs_ag],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î The hierarchical clustering technique did a pretty good job on all the datasets, but we had to do some tuning. Specifically, the choice of linkage can make a big difference, as can the choice of metric (which we did not tune here). We also provided the model with our desired number of clusters. Agglomerative Clustering will work without the number of clusters provided, but it might not produce the desired result. If you know the expected number of clusters, it is very helpful to provide it.\n‚ÑπÔ∏è Another thing you can do with hierarchical clustering is to visualize the relationships between the samples as a dendrogram. A dendrogram is a tree-like representation that shows every possible clustering from a single group to \\(N\\) groups (where \\(N\\) is the total number of samples). The height of the vertical lines represent changes in similarity between splits.\nYou can use this visualization to get an idea of how many clusters should exist in the dataset. This can be used to determine \\(k\\) before performing k-Means clustering, or for other models that require knowing the number of groups beforehand.\nüíÅ The dendrogram visualization is available from SciPy. (See: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram)\nTo see it in action, we will generate dendrogram for the ‚Äúpenguins‚Äù dataset:\n\nlinkage_matrix = linkage(penguins, \"ward\")\nplt.figure(figsize=(10, 9))\nplt.title(\"Hierarchical Penguins Dendrogram\")\nplt.ylabel(\"distance (Ward)\")\ndendrogram(linkage_matrix, no_labels=True, color_threshold=10)\nplt.show()\n\n\n\n\n\n\n\n\nBy examining the vertical lines, you can determine the ‚Äúbest‚Äù number of clusters by choosing where to ‚Äúcut‚Äù the graph with an imaginary horizontal line.\n\n\nGaussian Mixture Model\nThe Gaussian Mixture Model (GMM) works by assuming that clusters are multi-dimensional Gaussian distributions (which look like roughly ‚Äúcircular‚Äù blobs that are denser in the middle). It is quite flexible if you have ‚Äúblob-like‚Äù clusters, and GMM-like clusters occur frequently in natural datasets.\n\nfrom sklearn.mixture import GaussianMixture\n\n# Use a GMM to cluster each dataset.  We assume that penguins and\n# density blobs have three clusters, and that moons and circles have two.\n#\npenguins_ag = GaussianMixture(n_components=3).fit_predict(penguins)\nmoons_ag = GaussianMixture(n_components=2).fit_predict(moons)\ncircles_ag = GaussianMixture(n_components=2).fit_predict(circles)\ndensity_blobs_ag = GaussianMixture(n_components=3).fit_predict(density_blobs)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"GMM Penguins\",\n        \"GMM Moons\",\n        \"GMM Circles\",\n        \"GMM Density Blobs\",\n    ],\n    [penguins_ag, moons_ag, circles_ag, density_blobs_ag],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î The GMM did a very good job on the ‚Äúblob-like‚Äù datasets ‚Äúpenguins‚Äù and ‚Äúdensity blobs‚Äù. In fact, it probably did best on density blobs (versus the other methods we tried).\nHowever, it does not do as well with the datasets whose values aren‚Äôt ‚Äúblob-like‚Äù in shape."
  },
  {
    "objectID": "python_intermediate_clustering_complete_ASRI25.html#thank-you",
    "href": "python_intermediate_clustering_complete_ASRI25.html#thank-you",
    "title": "Clustering in Python (Intermediate)",
    "section": "Thank You!",
    "text": "Thank You!\nThis notebook in tutorial and completed form is available at:\nhttps://jcausey-astate.github.io/ASRI-2025/"
  }
]