{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b0514e9",
   "metadata": {
    "id": "df3301a1"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jcausey-astate/ASRI-2025/blob/main/python_intermediate_clustering_ASRI25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc63550",
   "metadata": {},
   "source": [
    "# Clustering in Python (Intermediate)\n",
    "\n",
    "## ASRI 2025\n",
    "\n",
    "![Classification in Python (Intermediate)](https://jcausey-astate.github.io/ASRI-2025/images/clustering_in_python_title_card.svg)\n",
    "\n",
    "This notebook shows some introductory examples from the \"Clustering in Python\"\n",
    "workshop session.\n",
    "\n",
    "The notebook uses the following modules:\n",
    "\n",
    "* `matplotlib`  : Provides basic graphing/charting.\n",
    "* `numpy`       : Allows matrix and vector/array math.\n",
    "* `pandas`      : Provides DataFrame functionality.\n",
    "* `scipy`       : SciPy provides algorithms for optimization, algebraic equations, statistics and many other classes of problems.  We will use it for building dendrograms.\n",
    "* `seaborn`     : Works with `matplotlib` to provide nicer graphs.\n",
    "* `sklearn`     : Scikit-Learn provides machine learning and data manipulation\n",
    "  tools.\n",
    "\n",
    "We will rely heavily on the Scikit-Learn library for models, metrics, and\n",
    "experimental design tools.  See the full documentation for this fantastic\n",
    "library at <https://scikit-learn.org>.\n",
    "\n",
    "---\n",
    "\n",
    "**Clustering** is an _unsupervised_ learning technique for exploring\n",
    "relationships between the random variables in a dataset.  We use a clustering\n",
    "analysis to try to identify groups or similar objects in datasets with two or\n",
    "more random variables.\n",
    "\n",
    "In this tutorial, we will look at some commonly-used clustering techniques:\n",
    "\n",
    "* **k-Means Clustering**\n",
    "  * Probably the most common clustering technique.\n",
    "  * Good for finding clusters that look like \"blobs\" when visualized.\n",
    "  * You need to know the number of clusters in advance.\n",
    "* **Mean Shift Clustering**\n",
    "  * Doesn't require assumptions about the number of clusters.\n",
    "  * Somewhat robust when clusters are not simple \"blobs\".\n",
    "* **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise)\n",
    "  * Clusters areas of similar _density_, regardless of cluster \"shape\".\n",
    "  * Works for situations where clusters don't look like separate \"blobs\".\n",
    "  * You don't need to know the number of clusters in advance.\n",
    "* **Agglomerative Hierarchical Clustering**\n",
    "  * Establishes relationships at all levels of the comparison metric between all\n",
    "    samples.\n",
    "  * Can be visualized as a _dendrogram_.\n",
    "  * Allows you to determine the \"right\" number of clusters by examining the\n",
    "    dendrogram.\n",
    "* **Gaussian Mixture Model**\n",
    "  * Assumes the data can be represented as some number of multi-dimensional\n",
    "    Gaussian distributions.\n",
    "  * Works well when the data are \"blob\" shaped, even when they overlap --\n",
    "    especially if the density of the blobs differ.\n",
    "  * Ideally, you need to know how many \"blobs\" to expect.\n",
    "\n",
    "‚ÑπÔ∏è  More information about these techniques and many others is available at\n",
    "<https://scikit-learn.org/stable/modules/clustering.html>.\n",
    "\n",
    "---\n",
    "\n",
    "## Let's see some code\n",
    "\n",
    "First, we have to import the modules, objects, and functions we will be using in\n",
    "this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that Scikit Learn is recent enough to include HDBSCAN:\n",
    "!pip install \"scikit-learn>=1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61aa3b7",
   "metadata": {
    "id": "d118e03b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn import cluster\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda4a24",
   "metadata": {
    "id": "fe75573c"
   },
   "source": [
    "## Comparing clustering techniques\n",
    "\n",
    "üí°  _There is no single perfect clustering algorithm._ Different algorithms make\n",
    "different assumptions with regards to what you already know about your dataset.\n",
    "\n",
    "For that reason, we will use a few different datasets to illustrate the\n",
    "strengths and weaknesses of each clustering algorithm.\n",
    "\n",
    "**Here are the datasets we will use:**\n",
    "\n",
    "* **Palmer Penguins** : This real-world dataset contains measurements of the\n",
    "  bill length versus the flipper length of three species of penguins at the Palmer\n",
    "  Antarctic research station.\n",
    "  <https://allisonhorst.github.io/palmerpenguins/articles/intro.html>\n",
    "* **Moons** : This simulated dataset consists of two crescent \"moon\" shapes,\n",
    "  with one inverted so that there is no way to linearly separate the two groups,\n",
    "  but they are visually separate.\n",
    "* **Circles** : This simulated dataset consists of two concentric rings,\n",
    "  representing noisy values that are in one of two groups, assigned by the\n",
    "  radius of the ring in which the sample is located.\n",
    "* **Density Blobs** : This simulated dataset will consist of three \"blobs\" with\n",
    "  widely varying density and size characteristics.  The blobs overlap so that\n",
    "  they are not perfectly visually separable.\n",
    "\n",
    "The following code block creates the simulated datasets and loads the Palmer\n",
    "Penguins dataset and selects the two predictors we will use for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11946a",
   "metadata": {
    "id": "cb956c4c"
   },
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "\n",
    "# penguins : this is a real dataset, so we will load it and select a useful set of\n",
    "#            features.  We will also store the labels.\n",
    "penguins = sns.load_dataset(\"penguins\").dropna(\n",
    "    axis=0, subset=[\"bill_length_mm\", \"flipper_length_mm\"]\n",
    ")\n",
    "penguins_encoder = LabelEncoder().fit(penguins[\"species\"].values)\n",
    "penguins_classes = penguins_encoder.classes_\n",
    "penguins_labels = penguins_encoder.transform(penguins[\"species\"].values)\n",
    "penguins = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].values\n",
    "penguins = StandardScaler().fit_transform(penguins)\n",
    "\n",
    "# moons : two crescent \"moon\" shapes, with one inverted so that there is no way to\n",
    "# linearly separate the two groups, but they are visually separate.\n",
    "moons, moons_labels = datasets.make_moons(n_samples=n_samples, noise=0.065)\n",
    "moons = StandardScaler().fit_transform(moons)\n",
    "\n",
    "# circles : two concentric rings, representing noisy values that are in one of two\n",
    "# groups, assigned by the radius of the ring in which the sample is located.\n",
    "circles, circles_labels = datasets.make_circles(\n",
    "    n_samples=n_samples, factor=0.5, noise=0.065\n",
    ")\n",
    "circles = StandardScaler().fit_transform(circles)\n",
    "\n",
    "# density blobs : three \"blobs\" with widely varying density and size characteristics.\n",
    "# The blobs overlap so that they are not perfectly visually separable.\n",
    "density_blobs, density_blobs_labels = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.9, 0.5, 2.7], random_state=4\n",
    ")\n",
    "density_blobs = StandardScaler().fit_transform(density_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4eefd9",
   "metadata": {
    "id": "c11ea67e"
   },
   "source": [
    "üí°  To make understanding the data easier, we need a way to visualize the\n",
    "datasets, plus give hints about what the \"expected\" labeling would be as well as\n",
    "what a clustering technique assigns as labels.\n",
    "\n",
    "We will create a function that can take a list of datasets with corresponding\n",
    "names, labels, and styles.  The function will use the Seaborn `scatterplot()`\n",
    "function to plot the datasets in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b75abc",
   "metadata": {
    "id": "9c8ba9b2"
   },
   "outputs": [],
   "source": [
    "def plot_all(data_list, names=None, label_list=None, style_list=None):\n",
    "    \"\"\"\n",
    "    Plot scatterplots for all the 2-D datasets in `data_list`.\n",
    "    `names` provide a title for each plot\n",
    "    `label_list` is a list of label assignments for the samples in the data list.\n",
    "    `style_list` is a list of marker shape assignments for the samples in the data list.\n",
    "    \"\"\"\n",
    "    nrows = int(round(len(data_list) / 2, 0))\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=2, figsize=(12, 10))\n",
    "    for r in range(nrows):\n",
    "        for c in range(2):\n",
    "            hue = label_list[r * 2 + c] if label_list is not None else None\n",
    "            style = style_list[r * 2 + c] if style_list is not None else None\n",
    "            title = names[r * 2 + c] if names is not None else None\n",
    "            sns.scatterplot(\n",
    "                x=data_list[r * 2 + c][:, 0],\n",
    "                y=data_list[r * 2 + c][:, 1],\n",
    "                hue=hue,\n",
    "                style=style,\n",
    "                palette=sns.color_palette(n_colors=len(set(hue))),\n",
    "                legend=False,\n",
    "                ax=axs[r, c],\n",
    "            ).set(title=title)\n",
    "    plt.subplots_adjust(wspace=0.50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7e86e",
   "metadata": {
    "id": "aad9ca84"
   },
   "source": [
    "Now, let's use the `plot_all()` function to plot our four datasets.  We will use\n",
    "the \"real\" labels to color the points so that you can see what the \"ideal\"\n",
    "labeling would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f9bdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "68433e62",
    "outputId": "fd7b8094-045d-41c0-9330-ea72ef2567a4"
   },
   "outputs": [],
   "source": [
    "plot_all(\n",
    "    [penguins, moons, circles, density_blobs],\n",
    "    [\"Penguins\", \"Moons\", \"Circles\", \"Density Blobs\"],\n",
    "    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053b24a",
   "metadata": {
    "id": "1e7d890f"
   },
   "source": [
    "üí°  **Keep in mind** that in many real-world clustering applications, we will\n",
    "_not_ know what the \"correct\" or \"ideal\" labels are.\n",
    "\n",
    "üí°  **That is the \"main idea\" of clustering:**  It is a technique for\n",
    "_suggesting a possible labeling_ for a dataset by examining relationships\n",
    "between the samples.\n",
    "\n",
    "### k-Means\n",
    "\n",
    "Let's start with **_k-Means_** clustering. **k-Means is probably the most well-known\n",
    "clustering technique because it is simple**, intuitive to understand, and it works\n",
    "well for clustering lots of datasets where we can visually identify \"groups\" in\n",
    "a scatterplot.\n",
    "\n",
    "k-Means works by first choosing a value $k$ that represents _the desired number\n",
    "of clusters_.  You need to already know something about the dataset to correctly\n",
    "choose $k$ (for example, you may visualize it first and choose the number that\n",
    "seem right from looking at the scatterplot).  There are also some techniques for\n",
    "choosing $k$ if you have no idea of what to try.  (See:\n",
    "<https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb>)\n",
    "\n",
    "Let's see how k-Means would label each of our test datasets.  We \"know\" from\n",
    "looking at the scatterplots that there are three groups in the \"penguins\" and\n",
    "\"density blobs\" datasets, and there are two groups in the \"moons\" and \"circles\"\n",
    "datasets.  So, we will choose $k$ for each dataset to match our expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea2022",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "6f51e7c6",
    "outputId": "954505f3-e69c-4422-89b6-958fe76e1544"
   },
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters=2, random_state=1)  # k-Means with 2 clusters\n",
    "kmeans_3 = KMeans(n_clusters=3, random_state=1)  # k-Means with 3 clusters\n",
    "\n",
    "# We apply k-Means to each dataset with the `fit_transform()` method.  We \"believe\"\n",
    "# that \"moons\" and \"circles\" should have k=2 and \"penguins\" and \"density_blobs\"\n",
    "# should have k=3.\n",
    "#\n",
    "penguins_kmeans = kmeans_3.fit_predict(penguins)\n",
    "moons_kmeans = kmeans_2.fit_predict(moons)\n",
    "# TODO: Complete the code here to perform the appropriate k-Means clustering on the\n",
    "# `circles` and `density_blobs` datasets.  See the variable names below and be sure\n",
    "# to store the results into the appropriately named variables.\n",
    "\n",
    "\n",
    "# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n",
    "# to determine the shapes of the markers in the plots here so that we can compare:\n",
    "#\n",
    "plot_all(\n",
    "    [penguins, moons, circles, density_blobs],\n",
    "    [\n",
    "        \"k-Means Penguins (k=3)\",\n",
    "        \"k-Means Moons (k=2)\",\n",
    "        \"k-Means Circles (k=2)\",\n",
    "        \"k-Means Density Blobs (k=3)\",\n",
    "    ],\n",
    "    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n",
    "    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df1b84",
   "metadata": {
    "id": "135e4563"
   },
   "source": [
    "ü§î  We can see that k-Means does a reasonable job of finding the three groups in the\n",
    "\"penguins\" dataset.  One of those groups (the Chinstrap penguins) is difficult\n",
    "to separate even with more complicated methods.  The method does a good job of\n",
    "separating the two \"easier\" groups (Adelie and Gentoo).\n",
    "\n",
    "‚ùì  **_What if we chose $k$ incorrectly?_**   Let's see by first assuming $k=2$\n",
    "for all four datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b71424",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "f0b2da82",
    "outputId": "52d6b1c9-2099-4251-ca6a-38fb482eed79"
   },
   "outputs": [],
   "source": [
    "# What if we believe there are 2 clusters in each of the datasets?\n",
    "#\n",
    "kmeans_2 = KMeans(n_clusters=2, random_state=1)  # k-Means with 2 clusters\n",
    "\n",
    "# We apply k-Means to each dataset with the `fit_transform()` method.\n",
    "#\n",
    "penguins_kmeans = kmeans_2.fit_predict(penguins)\n",
    "moons_kmeans = kmeans_2.fit_predict(moons)\n",
    "circles_kmeans = kmeans_2.fit_predict(circles)\n",
    "density_blobs_kmeans = kmeans_2.fit_predict(density_blobs)\n",
    "\n",
    "# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n",
    "# to determine the shapes of the markers in the plots here so that we can compare:\n",
    "#\n",
    "plot_all(\n",
    "    [penguins, moons, circles, density_blobs],\n",
    "    [\n",
    "        \"k-Means Penguins (k=2)\",\n",
    "        \"k-Means Moons (k=2)\",\n",
    "        \"k-Means Circles (k=2)\",\n",
    "        \"k-Means Density Blobs (k=2)\",\n",
    "    ],\n",
    "    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n",
    "    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f200d51",
   "metadata": {
    "id": "9ce8f1f0"
   },
   "source": [
    "‚ùì  What do you think about these clusters?\n",
    "\n",
    "Now, let's incorrectly assume $k=4$ for all four datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc3d23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "b372ca62",
    "outputId": "fad232c3-20ef-471f-a2c6-1336b539ad77"
   },
   "outputs": [],
   "source": [
    "# What if we believe there are 4 clusters in all of the datasets?\n",
    "#\n",
    "kmeans_4 = KMeans(n_clusters=4, random_state=1)  # k-Means with 4 clusters\n",
    "\n",
    "# We apply k-Means to each dataset with the `fit_transform()` method.\n",
    "#\n",
    "penguins_kmeans = kmeans_4.fit_predict(penguins)\n",
    "moons_kmeans = kmeans_4.fit_predict(moons)\n",
    "circles_kmeans = kmeans_4.fit_predict(circles)\n",
    "density_blobs_kmeans = kmeans_4.fit_predict(density_blobs)\n",
    "\n",
    "# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n",
    "# to determine the shapes of the markers in the plots here so that we can compare:\n",
    "#\n",
    "plot_all(\n",
    "    [penguins, moons, circles, density_blobs],\n",
    "    [\n",
    "        \"k-Means Penguins (k=4)\",\n",
    "        \"k-Means Moons (k=4)\",\n",
    "        \"k-Means Circles (k=4)\",\n",
    "        \"k-Means Density Blobs (k=4)\",\n",
    "    ],\n",
    "    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n",
    "    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af21db11",
   "metadata": {
    "id": "65350924"
   },
   "source": [
    "‚ÑπÔ∏è  With k-Means, the clustering is _stochastic_, meaning that it will be\n",
    "different each time you run it (unless you set the `random_state` to make it\n",
    "reproducible).\n",
    "\n",
    "**Let's see that in action:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e7cc51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "944173db",
    "outputId": "d3ad0fee-a879-4eba-94b3-a7d2edfbe769"
   },
   "outputs": [],
   "source": [
    "# What if we believe there are 4 clusters in all of the datasets?\n",
    "#\n",
    "kmeans_4 = KMeans(n_clusters=4)  # k-Means with 4 clusters, no fixed random state.\n",
    "\n",
    "# We apply k-Means to each dataset with the `fit_transform()` method.\n",
    "#\n",
    "penguins_kmeans_1 = kmeans_4.fit_predict(penguins)\n",
    "# TODO: repeat the line above three more times, changing the number\n",
    "# at the end of the variable name to 2, 3, 4 to denote each of four runs.\n",
    "\n",
    "\n",
    "# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n",
    "# to determine the shapes of the markers in the plots here so that we can compare:\n",
    "#\n",
    "plot_all(\n",
    "    [penguins, penguins, penguins, penguins],\n",
    "    [\n",
    "        \"k-Means Penguins (k=4)\",\n",
    "        \"k-Means Penguins (k=4)\",\n",
    "        \"k-Means Penguins (k=4)\",\n",
    "        \"k-Means Penguins (k=4)\",\n",
    "    ],\n",
    "    [penguins_kmeans_1, penguins_kmeans_2, penguins_kmeans_3, penguins_kmeans_4],\n",
    "    [penguins_labels, penguins_labels, penguins_labels, penguins_labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db9c64",
   "metadata": {
    "id": "32800edf"
   },
   "source": [
    "üí°  Keep in mind when using k-Means: A single run does not give you any\n",
    "indication confidence that the labeling was stable.  _(A stable labeling is an\n",
    "indication that the labeling is more likely \"correct\" with respect to some\n",
    "real-world relationship.)_\n",
    "\n",
    "You can run the clustering several times and compare the stability of the\n",
    "cluster assignments to get an idea of whether the algorithms is seeing \"real\"\n",
    "groups or just randomly assigning them based on the initial seed points chosen.\n",
    "\n",
    "For more information on measuring the stability of a clustering assignment, see\n",
    "the following article:\n",
    "\n",
    "<https://amueller.github.io/aml/04-model-evaluation/17-cluster-evaluation.html>\n",
    "\n",
    "### Mean-Shift Clustering\n",
    "\n",
    "Mean-Shift clustering provides a powerful technique for clustering a dataset\n",
    "when we _don't already know the number of clusters_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574b3de4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "8cd00eb4",
    "outputId": "1a8e50f8-b32e-4983-89da-61fbc181512b"
   },
   "outputs": [],
   "source": [
    "# For each dataset, we need to estimate the bandwidth parameter, then create the\n",
    "# MeanShift object.  To automate this, we will wrap the two steps up into a function.\n",
    "def build_MeanShift(X, quantile=0.25):\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=quantile)\n",
    "    return MeanShift(bandwidth=bandwidth)\n",
    "\n",
    "\n",
    "# Now build a MeanShift clusterer for each dataset and use it to fit/predict.\n",
    "penguins_ms = build_MeanShift(penguins).fit_predict(penguins)\n",
    "\n",
    "# TODO: write the code perform Mean-Shift on the `moons` data just like the line above.\n",
    "\n",
    "\n",
    "circles_ms = build_MeanShift(circles, quantile=0.2).fit_predict(circles)\n",
    "density_blobs_ms = build_MeanShift(density_blobs, quantile=0.1).fit_predict(\n",
    "    density_blobs\n",
    ")\n",
    "\n",
    "# Now plot them:\n",
    "plot_all(\n",
    "    [penguins, moons, circles, density_blobs],\n",
    "    [\n",
    "        \"Mean Shift Penguins\",\n",
    "        \"Mean Shift Moons\",\n",
    "        \"Mean Shift Circles\",\n",
    "        \"Mean Shift Density Blobs\",\n",
    "    ],\n",
    "    [penguins_ms, moons_ms, circles_ms, density_blobs_ms],\n",
    "    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b381cd",
   "metadata": {
    "id": "0ab5a434"
   },
   "source": [
    "ü§î  Mean-Shift did a very good job on the Penguins dataset, without needing to know\n",
    "that there were three groups!  It didn't do so well with the other datasets\n",
    "though.  Circles and Moons are not \"blob-like\" enough for this technique to work\n",
    "well.  In the density blobs dataset, it did (sort of) find the small dense\n",
    "blob... But at the expense of finding many more small clusters that don't really\n",
    "exist.\n",
    "\n",
    "‚ÑπÔ∏è  The `bandwidth` parameter is the most important tuning parameter for this\n",
    "algorithm, and it can be tricky to get just right.   The `estimate_bandwidth()`\n",
    "function is provided to help find a good bandwidth value, but it also has a\n",
    "tuning parameter `quantile` that can be a bit finicky.  Some trial-and-error may\n",
    "be required to get results that seem correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd01684",
   "metadata": {
    "id": "cd5ff026"
   },
   "source": [
    "### DBSCAN\n",
    "\n",
    "The DBSCAN technique looks for regions of _similar density_, and assumes that\n",
    "those regions represent groups of samples.  It is also capable of automatically\n",
    "identifying _outliers_ (or \"noise points\") that may not belong to any group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b4e93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "5e266bf9",
    "outputId": "ad8f7932-c76d-483a-b330-12d0706a965b"
   },
   "outputs": [],
   "source": [
    "# Like MeanShift, there is an important tuning parameter ('eps') for DBSCAN.\n",
    "# Use DBSCAN to cluster each dataset:\n",
    "penguins_DBSCAN = DBSCAN(eps=0.25).fit_predict(penguins)\n",
    "moons_DBSCAN = DBSCAN(eps=0.3).fit_predict(moons)\n",
    "circles_DBSCAN = DBSCAN(eps=0.28).fit_predict(circles)\n",
    "density_blobs_DBSCAN = DBSCAN(eps=0.1).fit_predict(density_blobs)\n",
    "\n",
    "# Now plot them:\n",
    "plot_all(\n",
    "    [penguins, moons, circles, density_blobs],\n",
    "    [\n",
    "        \"DBSCAN Penguins\",\n",
    "        \"DBSCAN Moons\",\n",
    "        \"DBSCAN Circles\",\n",
    "        \"DBSCAN Density Blobs\",\n",
    "    ],\n",
    "    [penguins_DBSCAN, moons_DBSCAN, circles_DBSCAN, density_blobs_DBSCAN],\n",
    "    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00ef33",
   "metadata": {
    "id": "e461b7c2"
   },
   "source": [
    "ü§î  We can see that DBSCAN did a very good job on the \"strange\" datasets (moons and\n",
    "circles).  It also did a decent job with the Penguins dataset.  The performance\n",
    "on the density blobs data is interesting: It identified the small dense cluster,\n",
    "but found spurious extra clusters as well.  It also failed to see the two larger\n",
    "clusters as separate objects.\n",
    "\n",
    "‚ÑπÔ∏è  DBSCAN requires careful tuning of the `eps` parameter, which is related to\n",
    "the expected density of clusters.  When there is a wide variation in the density\n",
    "of \"real\" clusters, DBSCAN can fail to perform well (as it did here).  However,\n",
    "if there is a sparse background of noisy points with some clusters of similar\n",
    "density, DBSCAN will find those clusters even if they are not \"blob-shaped\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d51d80",
   "metadata": {},
   "source": [
    "### HDBSCAN\n",
    "\n",
    "The HDBSCAN technique performs DBSCAN over varying epsilon (`eps`) values and integrates the result to find a clustering that gives the best stability over epsilon. This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN), and be more robust to parameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN isn't as sensitive to tuning parameters as DBSCAN, although tuning the \n",
    "# `min_cluster_size` can be helpful if you have an idea of how many values should\n",
    "# be contained in even the smallest cluster.\n",
    "# Use HDBSCAN to cluster each dataset:\n",
    "penguins_HDBSCAN = HDBSCAN().fit_predict(penguins)\n",
    "moons_HDBSCAN = HDBSCAN().fit_predict(moons)\n",
    "circles_HDBSCAN = HDBSCAN().fit_predict(circles)\n",
    "density_blobs_HDBSCAN = HDBSCAN().fit_predict(density_blobs)\n",
    "\n",
    "# Now plot them:\n",
    "plot_all(\n",
    "    [penguins, moons, circles, density_blobs],\n",
    "    [\n",
    "        \"HDBSCAN Penguins\",\n",
    "        \"HDBSCAN Moons\",\n",
    "        \"HDBSCAN Circles\",\n",
    "        \"HDBSCAN Density Blobs\",\n",
    "    ],\n",
    "    [penguins_HDBSCAN, moons_HDBSCAN, circles_HDBSCAN, density_blobs_HDBSCAN],\n",
    "    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070660af",
   "metadata": {
    "id": "79198b81"
   },
   "source": [
    "ü§î  HDBSCAN with defaults for all parameters did a pretty good job.  It was really good at the circles and moons, but introduced some spurious extra clusters in penguins and density blobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842599ef",
   "metadata": {
    "id": "79b9d147"
   },
   "source": [
    "### Agglomerative Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d0b7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "48fdfd8b",
    "outputId": "29fa70a7-a14f-459b-ff4e-fd9decb6c8fe"
   },
   "outputs": [],
   "source": [
    "# Use agglomerative clustering to cluster each dataset:\n",
    "penguins_ag = AgglomerativeClustering(n_clusters=3, linkage=\"ward\").fit_predict(\n",
    "    penguins\n",
    ")\n",
    "moons_ag = AgglomerativeClustering(n_clusters=2, linkage=\"single\").fit_predict(moons)\n",
    "\n",
    "# TODO: write the line to perform AgglomerativeClustering on the `circles` data.  Be sure\n",
    "# to select the correct number of clusters!\n",
    "\n",
    "\n",
    "density_blobs_ag = AgglomerativeClustering(n_clusters=3, linkage=\"ward\").fit_predict(\n",
    "    density_blobs\n",
    ")\n",
    "\n",
    "# Now plot them:\n",
    "plot_all(\n",
    "    [penguins, moons, circles, density_blobs],\n",
    "    [\n",
    "        \"Hierarchical Penguins\",\n",
    "        \"Hierarchical Moons\",\n",
    "        \"Hierarchical Circles\",\n",
    "        \"Hierarchical Density Blobs\",\n",
    "    ],\n",
    "    [penguins_ag, moons_ag, circles_ag, density_blobs_ag],\n",
    "    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b681fd6",
   "metadata": {
    "id": "7602d985"
   },
   "source": [
    "ü§î  The hierarchical clustering technique did a pretty good job on all the datasets,\n",
    "but we had to do some tuning.  Specifically, the choice of `linkage` can make a\n",
    "big difference, as can the choice of `metric` (which we did not tune here).  We\n",
    "also provided the model with our desired number of clusters.  Agglomerative\n",
    "Clustering will work _without_ the number of clusters provided, but it might not\n",
    "produce the desired result.  If you know the expected number of clusters, it is\n",
    "very helpful to provide it.\n",
    "\n",
    "‚ÑπÔ∏è  **Another** thing you can do with hierarchical clustering is to visualize the\n",
    "relationships between the samples as a **_dendrogram_**.  A dendrogram is a\n",
    "tree-like representation that shows every possible clustering from a single\n",
    "group to $N$ groups (where $N$ is the total number of samples).  The height of\n",
    "the vertical lines represent changes in similarity between splits.\n",
    "\n",
    "You can use this visualization to get an idea of how many clusters _should_\n",
    "exist in the dataset.  **This can be used to determine $k$ before performing\n",
    "k-Means clustering, or for other models that require knowing the number of\n",
    "groups beforehand.**\n",
    "\n",
    "üíÅ  The dendrogram visualization is available from SciPy. (See:\n",
    "<https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram>)\n",
    "\n",
    "To see it in action, we will generate dendrogram for the \"penguins\"\n",
    "dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_matrix = linkage(penguins, \"ward\")\n",
    "plt.figure(figsize=(10, 9))\n",
    "plt.title(\"Hierarchical Penguins Dendrogram\")\n",
    "plt.ylabel(\"distance (Ward)\")\n",
    "dendrogram(linkage_matrix, no_labels=True, color_threshold=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db400f2b",
   "metadata": {
    "id": "4cde7340"
   },
   "source": [
    "By examining the vertical lines, you can determine the \"best\" number of clusters\n",
    "by choosing where to \"cut\" the graph with an imaginary horizontal line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c118212e",
   "metadata": {
    "id": "5244d518"
   },
   "source": [
    "### Gaussian Mixture Model\n",
    "\n",
    "The Gaussian Mixture Model (GMM) works by assuming that clusters are\n",
    "multi-dimensional Gaussian distributions (which look like roughly \"circular\"\n",
    "blobs that are denser in the middle).  It is quite flexible if you have\n",
    "\"blob-like\" clusters, and GMM-like clusters occur frequently in natural\n",
    "datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f54829",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "b42ae769",
    "outputId": "73cc597d-e60c-4a13-8014-ee596c582485"
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Use a GMM to cluster each dataset.  We assume that penguins and\n",
    "# density blobs have three clusters, and that moons and circles have two.\n",
    "#\n",
    "penguins_ag = GaussianMixture(n_components=3).fit_predict(penguins)\n",
    "moons_ag = GaussianMixture(n_components=2).fit_predict(moons)\n",
    "circles_ag = GaussianMixture(n_components=2).fit_predict(circles)\n",
    "density_blobs_ag = GaussianMixture(n_components=3).fit_predict(density_blobs)\n",
    "\n",
    "# Now plot them:\n",
    "plot_all(\n",
    "    [penguins, moons, circles, density_blobs],\n",
    "    [\n",
    "        \"GMM Penguins\",\n",
    "        \"GMM Moons\",\n",
    "        \"GMM Circles\",\n",
    "        \"GMM Density Blobs\",\n",
    "    ],\n",
    "    [penguins_ag, moons_ag, circles_ag, density_blobs_ag],\n",
    "    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06477cb4",
   "metadata": {
    "id": "cc4a3146"
   },
   "source": [
    "ü§î  The GMM did a very good job on the \"blob-like\" datasets \"penguins\" and \"density\n",
    "blobs\".  In fact, it probably did _best_ on density blobs (versus the other\n",
    "methods we tried).\n",
    "\n",
    "However, it does not do as well with the datasets whose values aren't\n",
    "\"blob-like\" in shape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eaa2cf",
   "metadata": {},
   "source": [
    "## Thank You!\n",
    "\n",
    "This notebook in tutorial and completed form is available at:\n",
    "\n",
    "<https://jcausey-astate.github.io/ASRI-2025/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
